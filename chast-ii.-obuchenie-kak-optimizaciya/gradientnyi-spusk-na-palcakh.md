---
description: Почему производная – это направление движения.
---

# Градиентный спуск на пальцах

### Градиентный спуск на пальцах

Почему производная – это направление движения

Когда в машинном обучении говорят "обучение модели", почти всегда имеют в виду одно и то же: мы хотим подобрать параметры так, чтобы ошибка стала как можно меньше. Какая именно модель – линейная регрессия, логистическая, нейросеть – не так важно. Важнее то, что за кулисами почти всегда работает один и тот же механизм – градиентный спуск.

Слово звучит грозно, но идея у него на удивление простая. Настолько простая, что ее можно объяснить буквально "на пальцах".

### Ошибка как ландшафт

Представим себе очень простую ситуацию. У нас есть модель с одним параметром $$w$$. Мы меняем $$w$$ и каждый раз считаем ошибку $$L(w)$$. Например, это может быть MSE.

Если нарисовать график зависимости ошибки от $$w$$, то мы увидим кривую. Где-то ошибка большая, где-то меньше, а в одной точке – минимальная.

В этот момент полезно сменить абстракцию. Перестать думать об ошибке как о формуле и начать думать о ней как о ландшафте.

Значение параметра $$w$$ – это положение по горизонтали.

Значение ошибки $$L(w)$$ – это высота.

Мы как будто идем по гористой местности и хотим спуститься в самую низкую точку – в минимум.

\[PLACEHOLDER: картинка с холмом и точкой минимума, ось X – параметр w, ось Y – ошибка]

### Почему "спуск"

Если мы находимся на склоне холма, то интуитивно понятно, что делать. Нужно идти вниз. Но вот проблема: мы не знаем, где именно низина. У нас нет карты местности. Мы видим только то место, где стоим сейчас.

Все, что мы можем сделать – посмотреть под ноги и понять, куда склон уходит вниз сильнее.

Вот здесь и появляется производная.

### Производная как наклон

Производная функции $$L(w)$$ по $$w$$ – это, по сути, наклон кривой в текущей точке.

Формально:

$$
\frac{dL}{dw}
$$

Но без формального ада это означает следующее:

* если производная положительная, то при увеличении $$w$$ ошибка растет
* если производная отрицательная, то при увеличении $$w$$ ошибка уменьшается
* если производная равна нулю, то мы либо в минимуме, либо в максимуме, либо на плато

Геометрически производная – это тангенс угла наклона касательной к графику.

\[PLACEHOLDER: график функции и касательная, показывающая наклон]

### Почему производная – это направление движения

Допустим, мы стоим в точке $$w_0$$.

Если $$\frac{dL}{dw} > 0$$, значит график "идет вверх вправо". Чтобы спускаться, нам нужно двигаться влево, то есть уменьшать $$w$$.

Если $$\frac{dL}{dw} < 0$$, график “идет вниз вправо”. Значит, выгодно двигаться вправо, увеличивая $$w$$.

Обратите внимание на важный момент: мы не идем _по_ производной, мы идем _против_ нее.

Именно поэтому шаг градиентного спуска выглядит так:

$$
w_{new} = w_{old} - \eta \cdot \frac{dL}{dw}
$$

Здесь $$\eta$$ (эта буква называется "эта") – это скорость обучения, или learning rate.

Она отвечает за то, насколько длинный шаг мы делаем.

### Интуиция шага

Можно читать эту формулу буквально словами:

"Возьми текущее значение параметра и сдвинь его в сторону, противоположную наклону, на величину, пропорциональную этому наклону".

Если склон крутой, производная большая, и шаг получается больше.

Если склон пологий, производная маленькая, и шаг уменьшается.

Это очень похоже на то, как человек осторожно спускается с горы. На крутом участке он делает заметный шаг вниз. На почти ровном месте – еле переставляет ноги.

### Что будет, если шаг слишком большой

Интуиция подсказывает, что можно "ускориться" и взять очень большое значение $$\eta$$. Но тут нас поджидает классическая ловушка.

Если шаг слишком большой, мы не спускаемся, а начинаем перепрыгивать минимум. Сегодня – слева от него, завтра – справа, послезавтра – снова слева. Ошибка не уменьшается, а скачет.

\[PLACEHOLDER: график с перескакиванием минимума]

Если же шаг слишком маленький, обучение становится мучительно медленным. Мы вроде бы движемся в правильном направлении, но настолько медленно, что кажется, будто модель вообще не обучается.

Отсюда и главный практический вывод: learning rate – один из самых чувствительных параметров в обучении моделей.

### Много параметров – тот же принцип

До этого мы говорили про один параметр $$w$$. В реальных моделях параметров десятки, тысячи или миллионы.

Но идея не меняется.

Ошибка становится функцией многих переменных:

$$
L(w_1, w_2, \dots, w_n)
$$

Производная превращается в градиент – вектор из частных производных:

$$
\nabla L =
\left(
\frac{\partial L}{\partial w_1},
\frac{\partial L}{\partial w_2},
\dots,
\frac{\partial L}{\partial w_n}
\right)
$$

Этот вектор указывает направление _наибольшего роста_ функции ошибки. А значит, движение в противоположную сторону – это направление _наискорейшего убывания_.

И снова никакой магии. Мы просто идем вниз по самому крутому склону.

\[PLACEHOLDER: 3D-поверхность ошибки и стрелка градиента]

### Почему без производных никуда

Теперь становится понятно, почему производные – это не академическая прихоть, а рабочий инструмент.

Производная отвечает на простой вопрос:

"Если я слегка пошевелю параметры, в какую сторону и насколько изменится ошибка?"

Градиентный спуск – это не более чем повторение одного и того же цикла:

1. посчитай текущую ошибку
2. посчитай производные
3. сделай шаг в сторону уменьшения ошибки

Этот цикл и есть "обучение".

### Подведём итог

Если вынести за скобки формулы, градиентный спуск – это про ориентирование в пространстве ошибок. Производная – это наш компас. Она не говорит, где именно находится глобальный минимум, но всегда подсказывает, куда идти прямо сейчас.

И в этом смысле машинное обучение удивительно похоже на человеческий опыт. Мы редко знаем конечную цель в деталях. Но если мы понимаем, что становится хуже, а что – лучше, то можем шаг за шагом двигаться в правильном направлении.

Именно это и делает градиентный спуск.

---
description: Формула, геометрический смысл, PHP-реализация.
---

# Линейная регрессия как базовая модель

### Линейная регрессия как базовая модель

Линейная регрессия – это та точка, с которой удобно начинать разговор про машинное обучение. Не потому, что она "простая", а потому, что в ней уже есть почти всё: модель как функция, параметры, ошибка, оптимизация и геометрический смысл. Если понять линейную регрессию, дальше большинство моделей будут восприниматься как её усложнения.

### Идея модели

Представим, что у нас есть данные: входы и правильные ответы. Например, площадь квартиры и её цена. Мы хотим научиться по входу $$x$$ предсказывать значение $$y$$.

Линейная регрессия предполагает, что зависимость можно аппроксимировать линейной функцией:

$$
\hat{y} = w \cdot x + b
$$

Здесь:

* $$x$$ – входной признак
* $$w$$  – коэффициент (вес)
* $$b$$ – смещение (bias, свободный член)
* $$\hat{y}$$ – предсказание модели

Если признаков несколько, формула обобщается:

$$
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$

Или в векторной форме, которая важна для ML:

$$
\hat{y} = \mathbf{w} \cdot \mathbf{x} + b
$$

Здесь $$\mathbf{w}$$ и $$\mathbf{x}$$ – векторы, а точка означает скалярное произведение.

По сути, модель отвечает на вопрос: какие веса нужно подобрать, чтобы линейная комбинация признаков как можно лучше совпадала с реальными данными.

### Ошибка и функция потерь

Модель сама по себе ничего не значит, пока мы не определили, что такое "хорошо" и "плохо". Для этого вводится ошибка.

Для одного объекта ошибка выглядит так:

$$
e = y - \hat{y}
$$

Но оптимизировать просто ошибку неудобно – положительные и отрицательные значения будут взаимно уничтожаться. Поэтому почти всегда используют квадратичную ошибку:

$$
L = (y - \hat{y})^2
$$

А для всего датасета – среднеквадратичную ошибку (MSE):

$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$

Именно эту величину мы будем минимизировать, подбирая параметры $$w$$ и $$b$$.

### Геометрический смысл

Геометрия – ключ к пониманию линейной регрессии.

#### **Один признак – прямая**

Если у нас один признак, то данные – это точки на плоскости $$(x, y)$$. Модель – это прямая. Обучение линейной регрессии означает поиск такой прямой, которая проходит "как можно ближе" к этим точкам в смысле минимальной суммарной квадратичной ошибки.

<div align="left"><figure><img src="../../.gitbook/assets/11.1-points-and-approximating-line.png" alt="" width="563"><figcaption><p>11.1 Точки данных и аппроксимирующая прямая, которая минимизирует сумму квадратов вертикальных отклонений</p></figcaption></figure></div>

Вертикальные отрезки от точек до прямой – это и есть ошибки предсказания.

#### **Несколько признаков – плоскость и гиперплоскость**

Если признаков два, модель становится плоскостью. Если признаков больше – гиперплоскостью в многомерном пространстве.

Вектор $$\mathbf{w}$$ задаёт ориентацию этой плоскости, а $$b$$ – её сдвиг. Предсказание $$\hat{y}$$ – это проекция точки $$\mathbf{x}$$ на направление $$\mathbf{w}$$ с учётом смещения.

С этой точки зрения линейная регрессия – это задача подбора такого направления в пространстве признаков, которое лучше всего объясняет данные.

<div align="left"><figure><img src="../../.gitbook/assets/11.2-vectors-and-projection.png" alt="" width="563"><figcaption><p>11.2 Векторы x, w и проекция на направление w</p></figcaption></figure></div>

### Как находятся веса

Существует два основных подхода:

1. Аналитическое решение через нормальные уравнения
2. Итеративная оптимизация (градиентный спуск)

В прикладном ML чаще используется второй подход, потому что он масштабируется и логически совпадает с тем, как обучаются нейросети.

### Градиентный спуск – интуитивно

Идея простая: представим, что функция ошибки – это поверхность. Мы стоим в случайной точке и хотим спуститься в самую низкую.

Градиент показывает направление наибольшего роста функции. Если идти в противоположную сторону, ошибка будет уменьшаться.

Для линейной регрессии производные считаются просто:

$$
\frac{\partial L}{\partial w} = -2 x (y - \hat{y})
$$

$$
\frac{\partial L}{\partial b} = -2 (y - \hat{y})
$$

Обновление параметров выглядит так:

$$
w := w - \eta \frac{\partial L}{\partial w}
$$

$$
b := b - \eta \frac{\partial L}{\partial b}
$$

Где $$\eta$$ – learning rate, шаг обучения.

<div align="left"><figure><img src="../../.gitbook/assets/11.3-error-surface-and-gradient-descent.png" alt="" width="563"><figcaption><p>11.3 Поверхность ошибки и шаги градиентного спуска</p></figcaption></figure></div>

### Реализация на PHP – с нуля

Начнём с минимального примера: один признак, один вес. Оценка стоимости квартиры по её площади.

```php
// Обучающие данные
$x = [30, 40, 50, 60]; // площадь в м²
$y = [3, 4, 5, 6];     // цена (условно)

// Параметры модели
$w = 0.0; // вес
$b = 0.0; // смещение

// Параметры обучения
$learningRate = 0.0001;
$epochs = 5000;
$n = count($x);

// Градиентный спуск
for ($epoch = 0; $epoch < $epochs; $epoch++) {

    // Накопленные градиенты
    $dw = 0.0;
    $db = 0.0;

    // Проходим по всем точкам
    for ($i = 0; $i < $n; $i++) {
        // Предсказание модели
        $yPred = $w * $x[$i] + $b;
        
        // Ошибка предсказания
        // Если ошибка положительная – модель недооценила
        // Если отрицательная – переоценила
        $error = $y[$i] - $yPred;
    
        // Производные MSE по w и b (см. объяснение выше)
        $dw += -2 * $x[$i] * $error;
        $db += -2 * $error;
    }

    // Усредняем градиенты
    // Мы считаем средний градиент по всем точкам, а не делаем шаг после каждой. 
    // Это классический batch gradient descent.
    $dw /= $n;
    $db /= $n;

    // Обновляем параметры модели - шаг градиентного спуска
    // Мы двигаемся против направления градиента, потому что градиент указывает, куда ошибка растёт.
    // Маленький шаг – стабильное обучение.
    $w -= $learningRate * $dw;
    $b -= $learningRate * $db;
}

echo "w = {$w}, b = {$b}\n";

// Результат: w = 0.099958681685724, b = 0.0019740438781496
// Для этих данных результат будет близок к: y = 0.1x + 0
```

Этот код обучает простейшую линейную регрессию методом градиентного спуска.

Модель имеет вид:

$$
y = w \cdot x + b
$$

где

* $$x$$ – один признак (площадь),
* $$y$$ – целевая величина (цена),
* $$w$$ – вес (насколько цена растёт при увеличении площади),
* $$b$$ – смещение (базовая цена).

Код:

1. берёт набор пар $$(x, y)$$,
2. начинает с нулевых параметров $$w$$ и $$b$$,
3. много раз (epochs) считает ошибку,
4. вычисляет, как нужно изменить $$w$$ и $$b$$, чтобы ошибка уменьшалась,
5. постепенно сходится к значениям, которые лучше всего описывают данные.

В итоге мы получаем линию, максимально близкую к точкам. Важно, что здесь нет никакой "магии ML". Это обычный цикл, обычная математика и аккуратная работа с числами.

### Векторная версия

Когда признаков больше, удобнее мыслить векторами.

```php
// Скалярное произведение двух векторов
// Используется для вычисления предсказания ŷ = w · x
function dot(array $a, array $b): float {
    $sum = 0.0;
    foreach ($a as $i => $v) {
        $sum += $v * $b[$i];
    }
    return $sum;
}

// Матрица признаков X
// Каждая строка — один объект
// Первый элемент — реальный признак (площадь)
// Второй элемент всегда равен 1 — это bias, включённый как признак
$X = [
    [30, 1],
    [40, 1],
    [50, 1],
    [60, 1],
];

// Истинные значения (целевая переменная)
$y = [3, 4, 5, 6];

// Вектор весов модели
// w[0] — вес площади
// w[1] — вес bias (смещение)
$w = [0.0, 0.0];

// Параметры обучения
$learningRate = 0.0001;
$epochs = 1000;
$n = count($X);

// Градиентный спуск
for ($epoch = 0; $epoch < $epochs; $epoch++) {

    // Вектор градиентов для каждого веса
    $dw = [0.0, 0.0];

    // Проходим по всем объектам
    for ($i = 0; $i < $n; $i++) {

        // Предсказание: скалярное произведение весов и признаков
        $yPred = dot($w, $X[$i]);

        // Ошибка модели на текущем объекте
        $error = $y[$i] - $yPred;

        // Обновляем градиенты по каждому весу
        // ∂L/∂w_j = -2 * x_j * (y - ŷ)
        foreach ($dw as $j => $_) {
            $dw[$j] += -2 * $X[$i][$j] * $error;
        }
    }

    // Обновляем веса, двигаясь против градиента
    foreach ($w as $j => $_) {
        $w[$j] -= $learningRate * ($dw[$j] / $n);
    }
}

// Итоговые веса модели
print_r($w);

// Результат: 
// Array
// (
//   [0] => 0.099956715379918
//   [1] => 0.0020679870606752
// )
// Для этих данных результат будет близок к: y = 0.1x + 0
```

Здесь bias уже включён как дополнительный признак со значением 1.&#x20;

Это стандартный трюк, который часто используется в ML, потому что:

* одинаково работает для линейных моделей, логистической регрессии, SVM, нейросетей,
* упрощает backprop,
* позволяет хранить всё в матрицах,
* убирает условные ветки из кода и формул.

На практике почти все линейные модели в библиотеках делают именно так.

### Почему линейная регрессия так важна

Линейная регрессия кажется простой, но она:

* задаёт базовый шаблон "модель" → "ошибка" → "оптимизация"
* показывает геометрический смысл обучения
* учит мыслить векторами и пространствами
* напрямую связана с нейросетями (один нейрон – это линейная модель)

Фактически, каждый линейный слой в нейросети – это обобщение линейной регрессии. Разница лишь в количестве слоёв и нелинейностях между ними.

Если вы смогли понять, что происходит здесь, вы уже понимаете одну из важнейших частей машинного обучения – независимо от языка, фреймворка или модной библиотеки.

В следующих главах мы усложним картину и поговорим о том, почему линейности часто недостаточно и как появляются нелинейные модели.

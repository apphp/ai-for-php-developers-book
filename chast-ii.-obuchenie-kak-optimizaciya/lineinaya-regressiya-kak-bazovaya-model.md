---
description: Формула, геометрический смысл, PHP-реализация.
---

# Линейная регрессия как базовая модель

### Линейная регрессия как базовая модель

Линейная регрессия – это та точка, с которой удобно начинать разговор про машинное обучение. Не потому, что она «простая», а потому, что в ней уже есть почти всё: модель как функция, параметры, ошибка, оптимизация и геометрический смысл. Если понять линейную регрессию, дальше большинство моделей будут восприниматься как её усложнения.

### Идея модели

Представим, что у нас есть данные: входы и правильные ответы. Например, площадь квартиры и её цена. Мы хотим научиться по входу $$x$$ предсказывать значение $$y$$.

Линейная регрессия предполагает, что зависимость можно аппроксимировать линейной функцией:

$$
\hat{y} = w \cdot x + b
$$

Здесь:

* $$x$$ – входной признак
* $$w$$  – коэффициент (вес)
* $$b$$ – смещение (bias, свободный член)
* $$\hat{y}$$ – предсказание модели

Если признаков несколько, формула обобщается:

$$
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$

Или в векторной форме, которая важна для ML:

$$
\hat{y} = \mathbf{w} \cdot \mathbf{x} + b
$$

Здесь $$\mathbf{w}$$ и $$\mathbf{x}$$ – векторы, а точка означает скалярное произведение.

По сути, модель отвечает на вопрос: какие веса нужно подобрать, чтобы линейная комбинация признаков как можно лучше совпадала с реальными данными.

### Ошибка и функция потерь

Модель сама по себе ничего не значит, пока мы не определили, что такое «хорошо» и «плохо». Для этого вводится ошибка.

Для одного объекта ошибка выглядит так:

$$
e = y - \hat{y}
$$

Но оптимизировать просто ошибку неудобно – положительные и отрицательные значения будут взаимно уничтожаться. Поэтому почти всегда используют квадратичную ошибку:

$$
L = (y - \hat{y})^2
$$

А для всего датасета – среднеквадратичную ошибку (MSE):

$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$

Именно эту величину мы будем минимизировать, подбирая параметры $$w$$ и $$b$$.

### Геометрический смысл

Геометрия – ключ к пониманию линейной регрессии.

#### **Один признак – прямая**

Если у нас один признак, то данные – это точки на плоскости $$(x, y)$$. Модель – это прямая. Обучение линейной регрессии означает поиск такой прямой, которая проходит «как можно ближе» к этим точкам в смысле минимальной суммарной квадратичной ошибки.

\[КАРТИНКА 10.1: точки данных и несколько прямых, одна из которых минимизирует сумму квадратов вертикальных отклонений]

Вертикальные отрезки от точек до прямой – это и есть ошибки предсказания.

#### **Несколько признаков – плоскость и гиперплоскость**

Если признаков два, модель становится плоскостью. Если признаков больше – гиперплоскостью в многомерном пространстве.

Вектор $$\mathbf{w}$$ задаёт ориентацию этой плоскости, а $$b$$ – её сдвиг. Предсказание $$\hat{y}$$ – это проекция точки $$\mathbf{x}$$ на направление $$\mathbf{w}$$ с учётом смещения.

С этой точки зрения линейная регрессия – это задача подбора такого направления в пространстве признаков, которое лучше всего объясняет данные.

\[КАРТИНКА 10.2: векторы x, w и проекция на направление w]

### Как находятся веса

Существует два основных подхода:

1. Аналитическое решение через нормальные уравнения
2. Итеративная оптимизация (градиентный спуск)

В прикладном ML чаще используется второй подход, потому что он масштабируется и логически совпадает с тем, как обучаются нейросети.

### Градиентный спуск – интуитивно

Идея простая: представим, что функция ошибки – это поверхность. Мы стоим в случайной точке и хотим спуститься в самую низкую.

Градиент показывает направление наибольшего роста функции. Если идти в противоположную сторону, ошибка будет уменьшаться.

Для линейной регрессии производные считаются просто:

$$
\frac{\partial L}{\partial w} = -2 x (y - \hat{y})
$$

$$
\frac{\partial L}{\partial b} = -2 (y - \hat{y})
$$

Обновление параметров выглядит так:

$$
w := w - \eta \frac{\partial L}{\partial w}
$$

$$
b := b - \eta \frac{\partial L}{\partial b}
$$

Где $$\eta$$ – learning rate, шаг обучения.

\[КАРТИНКА 10.3: поверхность ошибки и шаги градиентного спуска]

### Реализация на PHP – с нуля

Начнём с минимального примера: один признак, один вес.

```php
$x = [30, 40, 50, 60]; // площадь
$y = [3, 4, 5, 6];    // цена (условно)

$w = 0.0;
$b = 0.0;
$learningRate = 0.001;
$epochs = 1000;
$n = count($x);

for ($epoch = 0; $epoch < $epochs; $epoch++) {
    $dw = 0.0;
    $db = 0.0;

    for ($i = 0; $i < $n; $i++) {
        $yPred = $w * $x[$i] + $b;
        $error = $y[$i] - $yPred;

        $dw += -2 * $x[$i] * $error;
        $db += -2 * $error;
    }

    $dw /= $n;
    $db /= $n;

    $w -= $learningRate * $dw;
    $b -= $learningRate * $db;
}

echo "w = {$w}, b = {$b}\n";
```

Этот код делает ровно то, что описано в формулах: считает ошибку, вычисляет градиенты и постепенно улучшает параметры.

Важно, что здесь нет никакой «магии ML». Это обычный цикл, обычная математика и аккуратная работа с числами.

### Векторная версия

Когда признаков больше, удобнее мыслить векторами.

```php
function dot(array $a, array $b): float {
    $sum = 0.0;
    foreach ($a as $i => $v) {
        $sum += $v * $b[$i];
    }
    return $sum;
}

$X = [
    [30, 1],
    [40, 1],
    [50, 1],
    [60, 1],
];

$y = [3, 4, 5, 6];

$w = [0.0, 0.0];
$learningRate = 0.001;
$epochs = 1000;
$n = count($X);

for ($epoch = 0; $epoch < $epochs; $epoch++) {
    $dw = [0.0, 0.0];

    for ($i = 0; $i < $n; $i++) {
        $yPred = dot($w, $X[$i]);
        $error = $y[$i] - $yPred;

        foreach ($dw as $j => $_) {
            $dw[$j] += -2 * $X[$i][$j] * $error;
        }
    }

    foreach ($w as $j => $_) {
        $w[$j] -= $learningRate * ($dw[$j] / $n);
    }
}

print_r($w);
```

Здесь bias уже включён как дополнительный признак со значением 1. Это стандартный трюк, который часто используется в ML.

### Почему линейная регрессия так важна

Линейная регрессия кажется простой, но она:

* задаёт базовый шаблон «модель → ошибка → оптимизация»
* показывает геометрический смысл обучения
* учит мыслить векторами и пространствами
* напрямую связана с нейросетями (один нейрон – это линейная модель)

Фактически, каждый линейный слой в нейросети – это обобщение линейной регрессии. Разница лишь в количестве слоёв и нелинейностях между ними.

Если вы понимаете, что происходит здесь, вы уже понимаете половину машинного обучения – независимо от языка, фреймворка или модной библиотеки.

В следующих главах мы усложним картину и поговорим о том, почему линейности часто недостаточно и как появляются нелинейные модели.

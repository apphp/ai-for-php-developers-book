---
description: MSE, log loss – без формального ада.
---

# Ошибка, loss-функции и зачем они нужны

### Ошибка, loss-функции и зачем они нужны

Любая модель машинного обучения сводится к простой идее: она пытается приблизить реальность функцией. А значит, между тем, что есть на самом деле, и тем, что говорит модель, всегда будет расхождение. Это расхождение мы и называем ошибкой.

Важно сразу понять одну вещь: модель не знает, что такое “хорошо” и “плохо”. Она не понимает смысл задачи. Всё, что она умеет – уменьшать число, которое мы ей дали. Это число и есть loss.

#### Ошибка как расстояние

Пусть у нас есть реальное значение y и предсказание модели $$\hat{y}$$. Самое естественное, что приходит в голову – посмотреть на разницу:

$$
e = y - \hat{y}
$$

Но такая ошибка неудобна. Она может быть отрицательной и положительной. Если у нас много объектов, ошибки начнут компенсировать друг друга.

Геометрически это выглядит так: мы смотрим, насколько далеко предсказание отстоит от реального значения на числовой прямой.

\[IMAGE: расстояние между y и ŷ на числовой оси]

По сути, ошибка – это расстояние. А значит, мы сразу приходим к идее: ошибка должна быть неотрицательной.

#### Квадрат ошибки как наказание за промах

Самый простой способ убрать знак – взять модуль. Но в ML почти всегда берут квадрат:

$$
(y - \hat{y})^2
$$

Почему?

Во-первых, квадрат делает функцию гладкой и удобной для оптимизации.

Во-вторых, квадрат усиливает большие ошибки.

Если ошибка выросла в 2 раза, штраф вырастает в 4 раза:

$$
(2e)^2 = 4e^2
$$

Это важный философский момент: мы заранее говорим модели, что редкие, но большие промахи хуже, чем много маленьких.

#### Mean Squared Error (MSE)

Если объектов много, мы усредняем квадраты ошибок:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

С точки зрения геометрии, MSE – это средний квадрат расстояния между точками и предсказаниями.

Если представить данные как точки на плоскости, а модель как линию или поверхность, MSE измеряет, насколько далеко точки находятся от этой поверхности.

\[IMAGE: точки данных и линия регрессии, вертикальные отрезки – ошибки]

#### Немного полезной математики

Почему MSE так часто используют? Потому что минимум MSE ведёт себя очень предсказуемо.

Если модель линейная:

$$
\hat{y} = wx + b
$$

то MSE как функция параметров w и b является выпуклой функцией. Это означает:

– у неё один глобальный минимум

– градиент всегда указывает в сторону улучшения

– обучение стабильно

\[IMAGE: график выпуклой функции потерь с единственным минимумом]

Это одна из причин, почему линейная регрессия – базовый и надёжный инструмент.

#### Связь MSE и нормального распределения

Есть важный, но часто неявный факт. Если мы используем MSE, то мы неявно предполагаем, что ошибки распределены нормально:

$$
\varepsilon = y - \hat{y} \sim \mathcal{N}(0, \sigma^2)
$$

В этом случае минимизация MSE эквивалентна максимизации правдоподобия.

Иначе говоря, MSE – это не просто удобная формула. Это зашитая гипотеза о природе шума в данных.

#### Почему MSE не подходит для классификации

Теперь представим задачу “да / нет”. Например, спам или не спам.

Реальное значение:

$$
y \in \{0, 1\}
$$

Предсказание модели:

$$
\hat{p} \in [0, 1]
$$

Если использовать MSE, то ошибка для 0.99 и 0.51 будет почти одинаковой, если правильный ответ – 1. Но интуитивно мы чувствуем, что это не одно и то же качество предсказания.

Нам важно не просто угадать, а насколько модель уверена.

#### Log loss как цена уверенности

Log loss решает именно эту проблему. Для одного объекта:

$$
\text{loss} = - \left( y \cdot \log(\hat{p}) + (1 - y) \cdot \log(1 - \hat{p}) \right)
$$

Если y = 1, остаётся только:

$$
-\log(\hat{p})
$$

График этой функции очень показателен.

\[IMAGE: график -log(p) при p → 0 и p → 1]

– при $$\hat{p} \to 1$$ ошибка стремится к нулю

– при $$\hat{p} \to 0$$ ошибка стремится к бесконечности

Это математическое выражение идеи:

> Быть уверенно неправым – почти преступление.

#### Геометрический смысл log loss

Log loss можно интерпретировать как расстояние между реальным распределением и предсказанным распределением вероятностей.

Формально это частный случай кросс-энтропии:

$$
H(p, q) = - \sum p \log q
$$

Где:

– $$p$$ – истинное распределение

– $$q$$ – распределение модели

\[IMAGE: два распределения вероятностей и расстояние между ними]

Это делает log loss естественным выбором для вероятностных моделей.

#### Сравнение MSE и log loss интуитивно

MSE спрашивает:

> Насколько далеко мы промахнулись по значению?

Log loss спрашивает:

> Насколько мы ошиблись в своей уверенности?

Именно поэтому:

– регрессия → MSE

– классификация → log loss

#### Итоговая мысль

Loss-функция – это единственный способ общения с моделью. Через неё мы объясняем, что считаем ошибкой, что считаем катастрофой, а что допустимым компромиссом.

Модель не знает ни денег, ни спама, ни смысла текста. Она знает только одно: куда двигаться, чтобы уменьшить loss.

В следующей главе мы увидим, как минимизация loss превращается в конкретный алгоритм обучения – через градиенты и обновление параметров.

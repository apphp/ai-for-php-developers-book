---
description: Почему данные – это точки в пространстве.
---

# Векторы, размерности и пространства признаков

Если убрать из машинного обучения все сложные слова и модные термины, то в сухом остатке почти всегда останется одна и та же идея: мы представляем реальные объекты числами и работаем с этими числами математически. Именно здесь на сцену выходят векторы, размерности и пространства признаков. Как PHP-разработчикам вам особенно важно понять это интуитивно, а не формально, потому что в коде вы постоянно будете иметь дело не с абстрактной линейной алгеброй, а с массивами чисел, матрицами и операциями над ними.

#### **Вектор как способ описать объект**

Вектор в контексте машинного обучения – это просто упорядоченный набор чисел. Каждое число описывает какой-то аспект объекта. Если объект простой, вектор короткий. Если объект сложный, вектор может быть очень длинным.

Представим пользователя интернет-магазина. Мы можем описать его так: возраст, количество покупок за год, средний чек. Тогда один пользователь – это вектор из трех чисел:

(возраст, покупки, средний чек)

В PHP это будет выглядеть максимально приземленно:

```php
$userVector = [35, 12, 78.5];
```

Важно понять: вектор – это не просто массив. Порядок элементов имеет смысл. Если вы перепутаете местами возраст и средний чек, модель не "догадается", что вы имели в виду. Для нее это будут другие данные.

#### **Размерность вектора**

Размерность вектора – это количество чисел в нем. В примере выше размерность равна 3. Если вы добавите еще один признак, например, "дней с последней покупки", размерность станет 4.

Размерность напрямую связана с тем, насколько подробно вы описываете объект. Низкая размерность означает грубое описание, высокая – более детальное. Но высокая размерность не всегда лучше. Каждый дополнительный признак – это новая степень свободы для модели и в то же время новый источник шума.

Для лучшего понимания, полезно думать о размерности как о фиксированном контракте. Если модель ожидает вектор длины 10, вы обязаны всегда передавать ровно 10 чисел, в одном и том же порядке.

```php
function predict(array $features): float {
    if (count($features) !== 10) {
        throw new InvalidArgumentException("Ожидается вектор размерности 10");
    }

    // дальнейшие вычисления
}
```

Пример:

```php
$features = [0.12, 0.85, 0.33, 0.67, 0.91, 0.44, 0.58, 0.76, 0.29, 0.50];

try {
    $result = predict($features);
    echo "Скор модели: " . round($result, 3) . PHP_EOL;
    
    // Интерпретация результата
    if ($result > 0.7) {
        echo "Высокая вероятность положительного исхода";
    } elseif ($result > 0.4) {
        echo "Средняя вероятность";
    } else {
        echo "Низкая вероятность";
    }
} catch (Exception $e) {
    echo "Ошибка: " . $e->getMessage();
}

// Скор модели: 0.75
// Высокая вероятность положительного исхода
```

#### **Пространство признаков**

Формально пространство признаков обычно обозначают как как $$R^n$$, что означает, что каждый объект описывается вектором $$x = (x₁, x₂, …, xₙ)$$ из $$n$$ вещественных чисел, а совокупность всех таких векторов образует единое абстрактное пространство. Именно в этом $$n$$-мерном пространстве "живут" данные, с которыми работают большинство моделей машинного обучения: в нём определены операции сложения векторов и умножения на число, что позволяет применять инструменты линейной алгебры. И хотя в практическом машинном обучении формальные аксиомы отходят на второй план, критически важно понимать, что любая модель всегда функционирует внутри фиксированного пространства, заданного структурой входных признаков.\
\
Если размерность равна 2, пространство признаков – это обычная плоскость. Если 3 – привычное трехмерное пространство. Если больше, то визуализировать его уже фактически невозможно, но математически это то же самое пространство $$R^n$$.

Каждая точка в этом пространстве – это один объект из реального мира, переведенный на язык чисел. Пользователь, товар, документ, изображение – все они после подготовки данных становятся точками в пространстве признаков.

Пространство признаков – это среда, в которой работает алгоритм машинного обучения. Всё, что раньше было строками, датами, категориями и JSON, после [feature engineering](../vvedenie/zaklyuchitelnye-materialy/glossarii.md#feature-engineering)  превращается в чистую математику – набор чисел.

#### **Признаки как оси координат**

Каждый признак – это отдельная ось координат в пространстве. Математически это означает, что значение признака $$xᵢ$$ – это координата точки вдоль $$i$$-й оси.

Например, вектор (35, 12, 78.5) – это точка в трехмерном пространстве, где первая ось – возраст, вторая – количество покупок, третья – средний чек.

Это сразу объясняет несколько важных вещей.

Во-первых, признаки должны быть сопоставимы по масштабу (ещё одна причина, почему мы используем преобразование признаков в числа). Если одна ось измеряется в десятках, а другая – в сотнях тысяч, то расстояния и углы, вычисляемые выбранной метрикой, перестают отражать реальное сходство объектов.

Во-вторых, добавление нового признака – это добавление новой оси. Пространство становится более размерным (плюс ещё один размер), а каждая точка получает еще одну координату. Модель вынуждена учитывать еще одно направление при принятии решений.

### **Нормализация и масштабирование**

На практике почти всегда приходится приводить признаки к сопоставимым масштабам по порядку величины. Это можно сделать разными способами: нормализацией (приведение значений к фиксированному диапазону, например от 0 до 1) или стандартизацией (приведение распределения к нулевому среднему и единичному стандартному отклонению). Оба подхода решают одну и ту же инженерную задачу – сделать признаки сопоставимыми для алгоритма.

В зависимости от свойств данных могут применяться логарифмические и другие нелинейные преобразования масштаба, однако их рассмотрение выходит за рамки данной книги.\
\
Нужно понять, что использование нормализации и масштабирования не прихоть математиков, а чисто инженерная необходимость, так как большинство алгоритмов машинного обучения чувствительны к масштабу входных данных: признаки с большими числовыми значениями начинают доминировать над остальными, искажая вклад действительно информативных признаков. Тем самым ухудшается качество и стабильность обучения модели.

#### **Нормализация**

Простейший пример – нормализация в диапазон от 0 до 1:

```php
function normalize(float $value, float $min, float $max): float {  
    $range = $max - $min;

    if ($range === 0.0) {
        return 0.0;
    }

    return ($value - $min) / $range;
}
```

Пример:

```php
$result = normalize(value: 75, min: 50, max: 100);
echo $result; 

// Результат: 0.5
// Объяснение: (75 - 50) / (100 - 50) = 0.5
```

После такой обработки возраст, количество покупок и средний чек начинают "весить" примерно одинаково в пространстве признаков.

#### **Стандартизация**

Другой широко используемый подход – стандартизация признаков. В отличие от нормализации, она не ограничивает значения фиксированным диапазоном, а приводит распределение признака к виду с нулевым средним и единичным стандартным отклонением. Это особенно важно для моделей, которые чувствительны к масштабу признаков (линейная и логистическая регрессия, SVM, нейронные сети), а также для методов оптимизации, использующих градиентный спуск. Стандартизация делает признаки сопоставимыми по масштабу, но при этом сохраняет информацию о выбросах и относительных отклонениях значений.

Простейшая формула стандартизации выглядит так:

```php
function standardize(float $value, float $mean, float $std): float {
    if ($std == 0.0) {
        return 0.0;
    }
    
    return ($value - $mean) / $std;
}
```

Пример:

```php
// Допустим, это признак "время ответа пользователя" (в секундах)
$value = 8.5;
// Статистика по обучающей выборке
$mean = 5.0;   // среднее значение
$std  = 2.0;   // стандартное отклонение

$zScore = standardize($value, $mean, $std);

echo "Z-score: " . round($zScore, 2) . PHP_EOL;

// Интерпретация
if ($zScore > 2) {
    echo "Значение сильно выше среднего (аномалия)";
} elseif ($zScore < -2) {
    echo "Значение сильно ниже среднего (аномалия)";
} elseif ($zScore > 1) {
    echo "Выше среднего";
} elseif ($zScore < -1) {
    echo "Ниже среднего";
} else {
    echo "В пределах нормы";
}

// Результат: 1.75
// Объяснение: (8.5 − 5.0) / 2.0 = 1.75
```

После стандартизации возраст, количество покупок и средний чек имеют среднее значение около нуля и сопоставимую дисперсию, благодаря чему обучение модели будет происходить стабильнее, а процесс оптимизации станет быстрее и предсказуемее.

### **Категориальные признаки и размерность**

Не все признаки изначально числовые. Цвет, страна, тип устройства – все это категории. Чтобы поместить их в пространство признаков, их нужно превратить в числа. Чаще всего это делается через [one-hot encoding](../vvedenie/zaklyuchitelnye-materialy/glossarii.md#one-hot-encoding).

Если у нас есть три возможных цвета: red, green, blue, то один признак превращается в три координаты:

```php
function encodeColor(string $color): array {
    return [
        $color === 'red' ? 1 : 0,
        $color === 'green' ? 1 : 0,
        $color === 'blue' ? 1 : 0,
    ];
}

echo 'Red: ' . encodeColor(color: 'red') . PHP_EOL;
echo 'Green: ' . encodeColor(color: 'green') . PHP_EOL;
echo 'Blue: ' . encodeColor(color: 'blue');

// Red:   [1, 0, 0]
// Green: [0, 1, 0]
// Blue:  [0, 0, 1]
```

Здесь важно заметить, что размерность имеет свойство резко расти. Один логический признак превратился в три числовых. В серьёзных задачах с сотнями категорий это становится серьезной проблемой и напрямую влияет на сложность моделей.

В реальных системах также приходится учитывать неизвестные или даже новые категории. Обычно для этого добавляют отдельный признак (например, "unknown"), который активируется, если значение не входит в обучающий словарь.

#### **Почему данные – это точки в пространстве**

Причина, по которой данные в машинном обучении рассматриваются как точки в пространстве, проста и фундаментальна. Большинство алгоритмов опираются на геометрию: расстояния, углы, проекции и поверхности.

Если у вас есть набор объектов, каждый из которых описан одним и тем же набором числовых признаков, то строгий математический способ работать с ними – рассматривать их как точки в одном и том же пространстве $$R^n$$ .

Формально: пусть каждый объект описывается вектором $$x = (x₁, x₂, …, xₙ)$$. Тогда вся выборка – это конечное множество точек $${x¹, x², …, xᵐ} ⊂ Rⁿ$$.

Расстояние между двумя точками $$||x − y||$$ отражает степень их сходства. Направление вектора $$(x − y)$$ показывает, в каких признаках объекты отличаются сильнее всего. Плоскость или гиперплоскость – это множество точек, удовлетворяющих линейному уравнению.

Именно поэтому даже самые разные модели в итоге сводятся к геометрическим операциям над векторами.

<div align="left"><figure><img src="../.gitbook/assets/6.1-points-in-2d-feature-space.png" alt="" width="563"><figcaption><p>8.1 Точки в 2D-пространстве признаков</p></figcaption></figure></div>

#### **Геометрический смысл расстояний и углов**

В пространстве признаков важны не только расстояния, но и углы между векторами.\
Угол между векторами показывает, насколько два направления изменений похожи.

Интуитивно можно сказать так: нас интересует не столько абсолютная величина изменений, сколько то, в каких признаках они происходят одновременно. Даже если значения сильно различаются по масштабу, малый угол между векторами означает, что признаки изменяются согласованно.

Именно поэтому косинусное сходство часто используется при работе с текстовыми  эмбеддингами и другими высокоразмерными данными, где направление вектора важнее его длины.

На практике такие векторы обычно предварительно нормализуют по длине (приводят к единичной норме), чтобы косинусное сходство отражало только направление векторов, а не их масштаб.

С формальной точки зрения всё это выражается через скалярное произведение. Скалярное произведение двух векторов $$x$$ и $$y$$ определяется как:

$$
x · y = Σᵢ xᵢ yᵢ
$$

Через него выражается косинус угла между векторами:

$$
cos(θ) = (x · y) / (||x|| · ||y||)
$$

Это напрямую используется, например, при сравнении текстовых эмбеддингов или пользовательских профилей.

<div align="left"><figure><img src="../.gitbook/assets/8.2-angle-between-2-vectors.png" alt="" width="563"><figcaption><p>8.2 Угол между двумя векторами</p></figcaption></figure></div>

С точки зрения машинного обучения это означает простую вещь: модель может считать два объекта похожими не потому, что они близки по всем координатам, а потому что они "смотрят" в одном направлении в пространстве признаков.

#### **Связь с конкретными алгоритмами**

Алгоритм k ближайших соседей (k-NN - k-Nearest Neighbors) буквально живет в пространстве признаков. Он ничего не обучает в классическом смысле, а просто для новой точки ищет k ближайших точек по выбранной метрике расстояния.

Другими словами, всё поведение этого алгоритма полностью определяется тем, как именно мы измеряем расстояния между точками.

Именно поэтому для алгоритма k-NN масштабирование признаков критично. Если признаки находятся в разных числовых диапазонах, расстояние между точками будет определяться в основном признаками с наибольшим масштабом, независимо от их реальной информативности.

Математически это выглядит так: для нового вектора x мы ищем такие векторы $$xᵢ$$ из обучающей выборки, для которых расстояние $$d(x, xᵢ)$$ минимально.

<div align="left"><figure><img src="../.gitbook/assets/6.3.-knn-in-feature-space.png" alt="" width="563"><figcaption><p>8.3 k-NN в пространстве признаков</p></figcaption></figure></div>

Функция указанная ниже реализует классическую формулу евклидова расстояния между двумя точками:

$$
d(a, b) = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}
$$

> Евклидово расстояние – самый интуитивный и часто используемый способ измерять расстояние между точками в пространстве признаков, но оно не является универсальным. В зависимости от задачи и природы данных могут применяться другие метрики: например, манхэттенское расстояние или косинусная мера сходства. Разные метрики по-разному определяют понятие "близости" между объектами, и выбор метрики напрямую влияет на поведение алгоритма и результаты модели.

В следующей главе мы рассмотрим функцию вычисления евклидова расстояния подробней.

Однако не все модели опираются на расстояния между точками. Линейные модели смотрят на пространство иначе. Они ищут гиперплоскость, которая лучше всего разделяет точки или аппроксимирует их значения.

Формально линейная модель записывается так:

$$
f(x) = \mathbf{w} \cdot \mathbf{x} + b
$$

Геометрически это означает, что все точки, для которых

$$
\mathbf{w} \cdot \mathbf{x} + b = 0
$$

лежат на одной гиперплоскости. Знак $$f(x)$$ определяет, по какую сторону границы находится точка.&#x20;

Если подобрать более "инженерную" формулировку, то можно написать так:

> Линейная модель разбивает пространство признаков гиперплоскостью, а знак линейной функции определяет класс объекта.

<div align="left"><figure><img src="../.gitbook/assets/6.4-linear-decision-boundary.png" alt="" width="563"><figcaption><p>8.4 Линейная граница принятия решений</p></figcaption></figure></div>

Функция ниже вычисляет:

$$
f(x) = \mathbf{w} \cdot \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b
$$

Геометрический смысл

* Значение функции пропорционально расстоянию точки до разделяющей гиперплоскости.
* Если $$f(x) = 0$$ – точка лежит на гиперплоскости.
* Если $$f(x) > 0$$ или $$f(x) < 0$$ – по разные стороны границы.

```php
function linearModel(array $x, array $w, float $b): float {
    $n = count($x);

    if ($n !== count($w)) {
        throw new InvalidArgumentException('x and w must have the same length');
    }

    $sum = $b;
    for ($i = 0; $i < $n; $i++) {
        $sum += $x[$i] * $w[$i];
    }

    return $sum;
}
```

Пример:

```php
$x = [2, 3];        // входные признаки
$w = [0.5, 1.5];    // веса
$b = 1.0;           // смещение (bias)

$result = linearModel($x, $w, $b);
echo $result;

// Результат: 6.5
// Объяснение: b + x[0] * w[0] + x[1] * w[1] = 1.0 + 2 * 0.5 + 3 * 1.5 = 6.5
```

Нейронные сети – это следующий шаг усложнения. Каждый слой выполняет [аффинное преобразование](../vvedenie/zaklyuchitelnye-materialy/glossarii.md#affinnoe-preobrazovanie):

$$
z = W x + b
$$

А затем применяет нелинейную функцию активации. Геометрически это означает, что пространство сначала линейно поворачивается и растягивается, а затем нелинейно "ломается". После нескольких таких преобразований данные, которые были неразделимы линейно, становятся разделимыми.

<div align="left"><figure><img src="../.gitbook/assets/6.5-nonlinear-transformation-of-space.png" alt="Нелинейное преобразование пространства" width="563"><figcaption><p>8.5 Нелинейной преобразование пространства</p></figcaption></figure></div>

Несмотря на сложность архитектур, вход у нейросети всегда один и тот же – вектор фиксированной размерности.

### **Высокая размерность и ее последствия**

Когда размерность пространства признаков становится большой, возникают эффекты, которые на интуитивном уровне кажутся странными. Объем пространства растет экспоненциально, точки становятся разреженными, а расстояния между ними выравниваются. Это часто называют "[проклятием размерности](https://en.wikipedia.org/wiki/Curse_of_dimensionality)".

Отсюда для разработчика напрашивается простой практический вывод: не добавляйте признаки "на всякий случай". Каждый признак должен иметь понятный смысл и пользу для задачи.

#### **Связь с реальными моделями**

Линейная регрессия, логистическая регрессия, нейронные сети – все они работают в пространстве признаков. Разница лишь в том, какие поверхности они могут в нем строить. Линейная модель проводит плоскость или гиперплоскость. Нейросеть – сложную нелинейную форму.

Но вход у них у всех один и тот же – вектор фиксированной размерности.

```php
$features = [0.42, 0.15, 0.78, 0.03];
$prediction = $model->predict($features);
```

Если вы четко понимаете, что означает каждый элемент этого массива и в каком пространстве он находится, половина проблем машинного обучения для вас уже решена.

### Ключевая мысль

Векторы – это язык, на котором данные разговаривают с моделями. Размерность – это сложность этого языка. Пространство признаков – это сцена, на которой разворачивается все машинное обучение. Для разработчика важно перестать видеть в этом абстрактную математику и начать видеть хорошо структурированные массивы чисел, за которыми стоят реальные свойства реальных объектов.

Вся дальнейшая математика машинного обучения строится вокруг этой геометрической интерпретации: данные – это точки, модель – это поверхность, а обучение – процесс поиска формы, которая лучше всего разделяет или аппроксимирует эти точки.

{% hint style="info" %}
Чтобы самостоятельно протестировать этот код, установите примеры из официального репозитория [GitHub](https://github.com/apphp/ai-for-php-developers-examples) или воспользуйтесь [онлайн-демонстрацией](https://aiwithphp.org/books/ai-for-php-developers/examples/part-1/vectors-dimensions-and-feature-spaces) для его запуска.
{% endhint %}

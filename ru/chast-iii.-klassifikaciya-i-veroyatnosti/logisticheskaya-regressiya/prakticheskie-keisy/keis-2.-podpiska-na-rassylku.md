# Кейс 2. Подписка на рассылку

Это самый простой сценарий в разделе – и именно поэтому он особенно важен.

Здесь нет многомерного пространства, нет сложных признаков и нет пересекающихся факторов. Есть один признак и одно решение.

И этого уже достаточно, чтобы увидеть всю логику логистической регрессии.

#### Цель кейса

Предсказать, подпишется ли пользователь на email-рассылку, исходя только из времени, проведенного на сайте.

Модель должна отвечать на два вопроса:

1. Какова вероятность подписки?
2. Где проходит граница между "скорее подпишется" и "скорее не подпишется"?

#### Сценарий

Предположим, у нас есть лендинг с формой подписки. Мы замечаем, что пользователи, которые проводят на странице больше времени, чаще оставляют email.

Упростим задачу максимально:

Признак:

* время на сайте (в минутах)

Целевая переменная:

* 1 – пользователь подписался
* 0 – пользователь не подписался

Каждый пользователь описывается одним числом:

```
x = [timeOnSite]
```

Это одномерная задача. Геометрически — точки на прямой.

#### Данные

Минимальный учебный датасет:

```php
use Rubix\ML\Classifiers\LogisticRegression;
use Rubix\ML\Datasets\Labeled;

$samples = [
    [0.5],
    [1.2],
    [2.0],
    [5.0],
    [7.0],
];

$labels = [0, 0, 0, 1, 1];

$dataset = new Labeled($samples, $labels);

$model = new LogisticRegression(0.1, 500);
$model->train($dataset);

$result = $model->predict([[3.0]]);
print_r($result);
```

В этих данных видно простую закономерность:

* меньше 3 минут – чаще не подписываются
* больше 5 минут – чаще подписываются

Модель должна уловить именно эту тенденцию.

#### Что делает модель

Логистическая регрессия в этом случае вычисляет:

$$
z = w \cdot x + b
$$

а затем применяет сигмоиду:

$$
p = \frac{1}{1 + e^{-z}}
$$

Поскольку признак один, формула предельно проста:

$$
z = w_1 \cdot time + b
$$

Decision boundary определяется условием: $$p = 0.5$$&#x20;

А это эквивалентно: $$w_1 \cdot x + b = 0$$

В одномерном случае это не линия, а одна точка на оси.

#### Визуализация

<div align="left"><figure><img src="../../../.gitbook/assets/14.5-newsletter-sigmoid-1d.png" alt="" width="563"><figcaption><p>14.5 График сигмоиды для подписки на рассылку</p></figcaption></figure></div>

#### Интерпретация

Это буквально сигмоида вдоль одной оси.

* Чем больше времени пользователь проводит на сайте, тем больше становится линейная комбинация $$z$$.
* Сигмоида плавно переводит её в вероятность.
* В какой-то точке вероятность пересекает 0.5 – это и есть граница решения.

Важно: модель не говорит "подпишется" или "не подпишется" сразу.

Она говорит, например:&#x20;

> Вероятность подписки: 0.63

И уже затем применяется порог (обычно 0.5).

#### Почему это важный пример

Этот кейс показывает логистическую регрессию в её чистом виде:

* одна переменная
* одна сигмоида
* одна точка decision boundary

Без лишней сложности становится очевидно:

1. Логистическая регрессия – это линейная модель.
2. Нелинейность появляется только из-за сигмоиды
3. Decision boundary – это условие $$z = 0$$.

#### Практический смысл

В реальном продукте такую модель можно использовать для:

* показа всплывающего окна подписки
* отправки дополнительного триггерного письма
* A/B-анализа порога времени

Даже один признак может давать полезный сигнал.

А логистическая регрессия позволяет превратить этот сигнал в управляемую вероятность.

#### Выводы

Этот кейс – минимальный, но показательный.

Он демонстрирует:

* как сигмоида превращает число в вероятность
* как появляется decision boundary
* как линейная модель становится инструментом принятия решений

Дальше мы будем добавлять признаки и измерения.

Но логика останется той же: линейная комбинация → сигмоида → вероятность → порог.

# Кейс 2. Подписка на рассылку

Это самый простой сценарий в разделе – и именно поэтому он особенно важен.

Здесь нет многомерного пространства, нет сложных признаков и нет пересекающихся факторов. Есть один признак и бинарное решение.

И этого уже достаточно, чтобы увидеть всю логику логистической регрессии.

#### Цель кейса

Предсказать, подпишется ли пользователь на email-рассылку, исходя только из времени, проведенного на сайте.

Модель должна отвечать на два вопроса:

1. Какова вероятность подписки?
2. Где проходит граница между "скорее подпишется" и "скорее не подпишется"?

#### Сценарий

Предположим, у нас есть лендинг с формой подписки. Мы замечаем, что пользователи, которые проводят на странице больше времени, чаще оставляют email.

Упростим задачу максимально:

Признак:

* время на сайте (в минутах)

Целевая переменная:

* 1 – пользователь подписался
* 0 – пользователь не подписался

Каждый пользователь описывается одним числом:

```
x = [timeOnSite]
```

Это одномерная задача. Геометрически — точки на прямой.

#### Данные

Минимальный учебный датасет:

```php
use Rubix\ML\Classifiers\LogisticRegression;
use Rubix\ML\Datasets\Unlabeled;
use Rubix\ML\Datasets\Labeled;

$samples = [
    [0.5],
    [1.2],
    [2.0],
    [5.0],
    [7.0],
];

$labels = ['no', 'no', 'no', 'yes', 'yes'];

$dataset = new Labeled($samples, $labels);

$model = new LogisticRegression();
$model->train($dataset);

$prediction = new Unlabeled([[3.0]]);
$labels = $model->predict($prediction);

echo "Predicted label: ";
print_r($labels);

// Show probabilities
$probas = $model->proba($prediction);
echo "\nProbabilities (per class): ";
print_r($probas[0]);

// Результат:
// Predicted label: Array (
//     [0] => yes
// )
// Probabilities (per class): Array (
//     [no] => 0.33716026803845
//     [yes] => 0.66283973196155
//  )
```

В данном запуске модель размещает decision boundary примерно в районе 3 минут (конкретное значение зависит от обученных параметров и регуляризации).

Поигравшись с тестовыми данными, можно заметить простую закономерность:

* малые значения времени (меньше 3 минут) чаще соответствуют классу "no"
* большие значения времени (больше 5 минут) чаще соответствуют классу "yes"

Модель должна уловить именно эту тенденцию. Чтобы понять, как именно она это делает, разберём формулу логистической регрессии.

#### Что делает модель

Логистическая регрессия в этом случае вычисляет:

$$
z = w \cdot x + b
$$

а затем применяет сигмоиду:

$$
p = \frac{1}{1 + e^{-z}}
$$

Поскольку признак один, формула предельно проста:

$$
z = w_1 \cdot time + b
$$

Поскольку модель линейна по признаку, зависимость z от времени представляет собой прямую.&#x20;

Decision boundary определяется условием: $$p = 0.5$$&#x20;

При стандартном пороге 0.5 это эквивалентно условию: $$w_1 \cdot x + b = 0$$

В одномерном случае это не линия, а одна точка на оси.

#### Визуализация

<div align="left"><figure><img src="../../../.gitbook/assets/14.5-newsletter-sigmoid-1d.png" alt="" width="563"><figcaption><p>14.5 График сигмоиды для подписки на рассылку</p></figcaption></figure></div>

#### Интерпретация

Это график вероятности подписки как функции времени – он имеет форму сигмоиды.

* Чем больше времени пользователь проводит на сайте, тем больше становится линейная комбинация $$z$$.
* Сигмоида плавно переводит её в вероятность.
* В какой-то точке вероятность пересекает 0.5 – это и есть граница решения.

Важно: модель не говорит "подпишется" или "не подпишется" сразу.

Она говорит, например:&#x20;

> Вероятность подписки: 0.63

И уже затем применяется порог (обычно 0.5).

#### Почему это важный пример

Этот кейс показывает логистическую регрессию в её чистом виде:

* одна переменная
* одна сигмоида
* одна точка decision boundary

Без лишней сложности становится очевидно:

1. Логистическая регрессия – это линейная модель.
2. Нелинейность появляется только из-за сигмоиды
3. При стандартном пороге 0.5 decision boundary – это условие $$z = 0$$.

#### Практический смысл

В реальном продукте такую модель можно использовать для:

* показа всплывающего окна подписки
* отправки дополнительного триггерного письма
* A/B-анализа порога времени

Даже один признак может давать полезный сигнал. При условии, что между признаком и целевой переменной существует устойчивая зависимость.А логистическая регрессия позволяет превратить этот сигнал в управляемую вероятность.

#### Выводы

Этот кейс – минимальный, но показательный.

Он демонстрирует:

* как сигмоида превращает число в вероятность
* как появляется decision boundary
* как линейная модель становится инструментом принятия решений

Дальше мы будем добавлять признаки и измерения.

Но логика останется той же: линейная комбинация → сигмоида → вероятность → порог.

{% hint style="info" %}
Чтобы самостоятельно протестировать этот код, установите примеры из официального репозитория [GitHub](https://github.com/apphp/ai-for-php-developers-examples) или воспользуйтесь [онлайн-демонстрацией](https://aiwithphp.org/books/ai-for-php-developers/examples/part-3/logistic-regression) для его запуска.
{% endhint %}

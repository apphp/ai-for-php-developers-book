# Кейс 5. Мини-GPT на PHP как демонстрационный проект

#### Идея

Создать мини-демо-проект для книги:

“LLM за 200 строк PHP”

Что внутри:

1. Токенизация
2. Подсчёт n-грамм
3. Cross-entropy
4. Temperature
5. Генерация текста

#### Архитектура

```
Input → Tokens → Probability table → Softmax-like sampling → Output
```

### Педагогический эффект

Разработчик видит:

* нет магии
* нет “интеллекта”
* только статистика
* но статистика + масштаб = GPT


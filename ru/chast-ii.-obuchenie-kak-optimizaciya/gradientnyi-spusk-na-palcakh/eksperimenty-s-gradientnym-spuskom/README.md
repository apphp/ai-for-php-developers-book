# Эксперименты с градиентным спуском

#### Зачем смотреть внутрь обучения

До этого момента мы рассматривали модели как законченные объекты. Есть данные, есть формула, есть результат. Линейная регрессия, например, выглядит почти статично в готовом виде: мы подставили признаки, получили коэффициенты, сделали прогноз.

Но в реальности модель не появляется из ниоткуда. Прежде чем коэффициенты станут "правильными", они проходят длинный путь. Этот путь и есть обучение.

Градиентный спуск – не просто формула и не абстрактный алгоритм, а процесс. Это последовательность маленьких шагов, в каждом из которых параметры модели немного изменяются в сторону уменьшения ошибки. Чтобы понять его по-настоящему, недостаточно увидеть итоговые коэффициенты. Нужно посмотреть, как они движутся.

#### От моделей к движению параметров

Именно поэтому в этой части книги мы временно забудем про бизнес-задачи и прикладные сценарии. Вместо этого мы будем наблюдать сам процесс обучения: как меняется параметр от шага к шагу, как ведет себя ошибка, что происходит при удачном и неудачном выборе скорости обучения, и почему иногда модель будто бы "застревает".

Эти эксперименты намеренно просты. В них нет сложных датасетов, хитрых признаков или продвинутых моделей. Зато в них максимально прозрачно видно то, что обычно скрыто внутри метода `train()`: направление движения, роль производной и влияние каждого шага.

Цель этой главы – сформировать интуицию. После нее градиентный спуск перестает быть магией и начинает восприниматься как обычный, почти механический процесс. А когда понятен процесс, становятся понятны и его ограничения.

В следующих разделах мы последовательно разберем несколько характерных экспериментов:

* Пример 1. Как параметр модели шаг за шагом движется к минимуму ошибки
* Пример 2. Как выбор [learning rate](../../../vvedenie/zaklyuchitelnye-materialy/glossarii.md#learning-rate-skorost-obucheniya) влияет на скорость и устойчивость обучения
* Пример 3. Что происходит, когда градиент становится почти нулевым и возникает эффект плато
* Пример 4. Чем отличается batch gradient descent от стохастического и почему шум в обучении – это нормально.

Каждый из этих экспериментов будет реализован на чистом PHP и снабжен комментариями, чтобы можно было буквально увидеть и почувствовать, как работает градиентный спуск изнутри.

# Кейс 3. Log loss и уверенность классификатора

В этом кейсе мы впервые выходим за пределы "угадывает / не угадывает" и переходим к вероятностному мышлению. Именно здесь становится ясно, почему для классификации почти всегда используют log loss, а не MSE.

#### **Цель кейса**

Показать, что в задачах классификации важна не только правильность ответа, но и степень уверенности модели, и что log loss умеет учитывать это свойство, тогда как MSE — нет.

#### **Сценарий**

Рассмотрим простейший спам-фильтр. На вход подаётся письмо, на выходе модель возвращает не класс, а вероятность того, что письмо является спамом.

* 1 означает "спам"
* 0 означает "не спам"

Реальные метки писем:

```php
$y = [1, 0, 1, 0];
```

Предсказанные вероятности:

```php
$p = [0.95, 0.1, 0.55, 0.4];
```

На первый взгляд всё выглядит неплохо. Если поставить порог, скажем, 0.5, модель в каждом случае угадывает класс. Но интуитивно мы чувствуем: предсказание 0.95 и 0.55 – это разный уровень качества, даже если оба формально верны.

#### **Как считается log loss**

Реализуем log loss в чистом PHP для:

$$
\text{LogLoss} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
$$

где

* $$y_i$$ истинная метка (0 или 1),&#x20;
* а – $$p_i$$ предсказанная вероятность класса 1.

```php
function logLoss(array $y, array $p): float {
    $n = count($y);
    $sum = 0.0;

    for ($i = 0; $i < $n; $i++) {
        $pi = min(max($p[$i], 1e-15), 1 - 1e-15);
        $sum += -($y[$i] * log($pi) + (1 - $y[$i]) * log(1 - $pi));
    }

    return $sum / max($n, 1);
}

echo logLoss($y, $p);
// Результат:
// ≈ 0.316339
```

Важно обратить внимание на ограничение вероятностей. Ограничение вероятностей защищает от `log(0)`, который математически стремится к бесконечности. Уже здесь можно увидеть ключевую идею log loss: уверенная ошибка наказывается чрезвычайно сильно.

#### **Что именно наказывает log loss**

Рассмотрим два случая, когда правильный ответ – 1 (спам):

* вероятность 0.95
* вероятность 0.55

Обе классификации корректны, но штрафы различаются:

$$
-\log(0.95) \approx 0.05
$$

и

$$
-\log(0.55) \approx 0.60
$$

Модель, которая сомневается, платит большую цену. А если модель уверенно ошибается и выдаёт 0.01, штраф становится почти катастрофическим:

$$
-\log(0.01) \approx 4.6
$$

#### **Ключевая идея log loss**

Log loss кодирует очень конкретное правило:

> Ошибаться можно, но быть уверенно неправым – нельзя.

Это принципиально отличает log loss от MSE. Хотя MSE численно различает такие ошибки, он не делает качественного различия между умеренно и чрезмерно уверенными предсказаниями. Для MSE это всего лишь разные расстояния до цели, тогда как log loss резко наказывает именно уверенную ошибку.

#### **Почему модель "учится сомневаться"**

Здесь важно понять, что log loss — это не просто эвристическая функция потерь. Формально он является отрицательным логарифмическим правдоподобием (negative log-likelihood) для бернуллиевского распределения.

Используя log loss, мы фактически предполагаем, что:

* целевая переменная бинарна,
* модель предсказывает параметр распределения — вероятность события,
* обучение сводится к максимизации правдоподобия наблюдаемых данных.

Вероятность, близкая к нулю для истинного класса, делает правдоподобие почти нулевым, а логарифм — большим по модулю отрицательным числом. Поэтому модель вынуждена:

* повышать уверенность только там, где она действительно оправдана,
* избегать крайних вероятностей без достаточных оснований,
* калибровать свои предсказания.

В результате мы получаем не просто классификатор, а вероятностную модель, которой можно доверять.

#### **Почему здесь нельзя использовать MSE**

Если заменить log loss на MSE, обучение будет происходить под квадрат разницы между $$y$$ и $$p$$. Это соответствует гауссовскому предположению о шуме и не является корректной вероятностной моделью для бинарной классификации.

В результате:

* уверенность влияет на штраф слишком мягко,
* вероятности теряют строгий статистический смысл,
* модель перестаёт различать "чуть уверен" и "почти уверен" в вероятностном смысле.

MSE не различает философию уверенности, а log loss – различает.

<details>

<summary>Почему MSE не различает степень уверенности</summary>

Чтобы понять это, рассмотрим тот же пример бинарной классификации. Пусть правильный ответ – 1 (спам).

Возьмём два вероятностных предсказания<br>

* $$\hat{p} = 0.99$$
* $$\hat{p} = 0.51$$

Обе вероятности приводят к правильному классу при пороге 0.5, но интуитивно они очень разные.

Посмотрим, как их оценивает MSE.

Для бинарной цели $$y = 1$$ MSE считается так: $$\text{MSE} = (1 - \hat{p})^2$$

Подставим значения:

* $$(1 - 0.99)^2 = 0.0001$$
* $$(1 - 0.51)^2 = 0.2401$$

Теперь сравним это с log loss.

Для $$y = 1$$: $$\text{log loss} = -\log(\hat{p})$$

Подставим значения:

* $$-\log(0.99) \approx 0.01$$
* $$-\log(0.51) \approx 0.67$$

**Что здесь принципиально различается**

Посмотрим не на сами числа, а на характер штрафа.

* у MSE штраф растёт квадратично и довольно мягко
* у log loss штраф логарифмический и стремится к бесконечности при уверенной ошибке

Особенно важно поведение при уверенной ошибке.

Возьмём $$\hat{p}$$  = 0.01 при y = 1:

* $$\text{MSE} = (1 - 0.01)^2 \approx 0.98$$
* $$\text{log loss} = -\log(0.01) \approx 4.6$$

Для MSE это всего лишь значение, сравнимое с другими большими ошибками.

Для log loss – почти катастрофа.

**Интуитивный вывод**

MSE действительно различает 0.99 и 0.51, но делает это слишком мягко для вероятностной интерпретации. Log loss же:

* резко наказывает за уверенную ошибку
* поощряет осторожные вероятности
* формирует калиброванное распределение

Поэтому:

> MSE учитывает величину ошибки, но слабо отражает цену уверенности.

> Log loss делает уверенность центральной частью штрафа.

</details>

#### **Выводы**

В этом кейсе мы видим, что:

* log loss учитывает не только факт ошибки, но и уверенность
* одинаково "правильные" ответы могут иметь разное качество
* минимизация log loss формирует калиброванные вероятности

Именно поэтому log loss является стандартом для задач классификации. В следующем кейсе мы сделаем ещё один шаг и увидим, как две модели с одинаковой точностью могут иметь принципиально разное качество с точки зрения функции потерь.

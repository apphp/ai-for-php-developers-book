# Кейс 3. Log loss и уверенность классификатора

В этом кейсе мы впервые выходим за пределы "угадывает / не угадывает" и переходим к вероятностному мышлению. Именно здесь становится ясно, почему для классификации почти всегда используют log loss, а не MSE.

#### **Цель кейса**

Показать, что в задачах классификации важна не только правильность ответа, но и степень уверенности, и что log loss умеет это учитывать, а другие функции потерь – нет.

#### **Сценарий**

Рассмотрим простейший спам-фильтр. На вход подаётся письмо, на выходе модель возвращает не класс, а вероятность того, что письмо является спамом.

* 1 означает "спам"
* 0 означает "не спам"

Реальные метки писем:

```php
$y = [1, 0, 1, 0];
```

Предсказанные вероятности:

```php
$p = [0.95, 0.1, 0.55, 0.4];
```

На первый взгляд всё выглядит неплохо. Если поставить порог, скажем, 0.5, модель в каждом случае угадывает класс. Но интуитивно мы чувствуем: предсказание 0.95 и 0.55 – это разный уровень качества, даже если оба формально верны.

#### **Как считается log loss**

Реализуем log loss в чистом PHP:

```php
function logLoss(array $y, array $p): float {
    $n = count($y);
    $sum = 0.0;

    for ($i = 0; $i < $n; $i++) {
        $pi = min(max($p[$i], 1e-15), 1 - 1e-15);
        $sum += -($y[$i] * log($pi) + (1 - $y[$i]) * log(1 - $pi));
    }

    return $sum / max($n, 1);
}

echo logLoss($y, $p);
// Результат:
// 
```

Важно обратить внимание на ограничение вероятностей. Мы защищаемся от `log(0)`, который математически стремится к бесконечности. И это уже подсказка к смыслу функции.

#### **Что именно наказывает log loss**

Рассмотрим два случая, когда правильный ответ – 1 (спам):

* вероятность 0.95
* вероятность 0.55

Обе классификации корректны, но штрафы разные:

первый вариант

$$
-\log(0.95) \approx 0.05
$$

и второй вариант

$$
-\log(0.55) \approx 0.60
$$

Модель, которая сомневается, платит большую цену. А теперь представим, что модель уверенно ошиблась и выдала 0.01:

$$
-\log(0.01) \approx 4.6
$$

Штраф становится огромным.

#### **Ключевая идея log loss**

Log loss кодирует очень конкретное правило:

> Ошибаться можно, но быть уверенно неправым – нельзя.

Это принципиально отличается от log loss: хотя MSE численно различает такие ошибки, он не делает качественного различия между умеренно и чрезмерно уверенными предсказаниями. Для MSE это всего лишь разные расстояния до цели, тогда как log loss резко наказывает именно уверенную ошибку и тем самым заставляет модель быть честной в своих вероятностях.

#### **Почему модель "учится сомневаться"**

Минимизируя log loss, модель вынуждена:

* повышать уверенность только там, где она действительно оправдана
* избегать крайних вероятностей без достаточных оснований
* калибровать свои предсказания

В результате мы получаем не просто классификатор, а вероятностную модель, которой можно доверять.

#### **Почему здесь нельзя использовать MSE**

Если заменить log loss на MSE, модель будет оптимизироваться под квадрат разницы между $$y$$ и $$p$$. Это приведёт к странному поведению:

* уверенность почти не влияет на штраф
* вероятности теряют смысл
* модель перестаёт различать "чуть уверен" и "почти уверен"

MSE не различает философию уверенности, а log loss – различает.

<details>

<summary>Почему MSE не различает степень уверенности</summary>

Чтобы понять это, рассмотрим тот же пример бинарной классификации. Пусть правильный ответ – 1 (спам).

Возьмём два вероятностных предсказания<br>

* $$\hat{p} = 0.99$$
* $$\hat{p} = 0.51$$

Обе вероятности приводят к правильному классу при пороге 0.5, но интуитивно они очень разные.

Посмотрим, как их оценивает MSE.

Для бинарной цели $$y = 1$$ MSE считается так: $$\text{MSE} = (1 - \hat{p})^2$$

Подставим значения:

* $$(1 - 0.99)^2 = 0.0001$$
* $$(1 - 0.51)^2 = 0.2401$$

Теперь сравним это с log loss.

Для $$y = 1$$: $$\text{log loss} = -\log(\hat{p})$$

Подставим значения:

* $$-\log(0.99) \approx 0.01$$
* $$-\log(0.51) \approx 0.67$$

**Что здесь принципиально различается**

Посмотрим не на сами числа, а на характер штрафа.

* у MSE штраф растёт квадратично и довольно мягко
* у log loss штраф растёт логарифмически и резко около нуля

Особенно важно поведение при уверенной ошибке.

Возьмём $$\hat{p}$$  = 0.01 при y = 1:

* $$\text{MSE} = (1 - 0.01)^2 \approx 0.98$$
* $$\text{log loss} = -\log(0.01) \approx 4.6$$

Для MSE это всего лишь значение, сравнимое с другими большими ошибками.

Для log loss – почти катастрофа.

**Интуитивный вывод**

MSE действительно различает 0.99 и 0.51, но делает это слишком мягко для вероятностной интерпретации. Log loss же:

* резко наказывает за уверенную ошибку
* поощряет осторожные вероятности
* формирует калиброванное распределение

Поэтому:

> MSE учитывает величину ошибки, но слабо отражает цену уверенности.

> Log loss делает уверенность центральной частью штрафа.

</details>

#### **Выводы**

В этом кейсе мы видим, что:

* log loss учитывает не только факт ошибки, но и уверенность
* одинаково "правильные" ответы могут иметь разное качество
* минимизация log loss формирует калиброванные вероятности

Именно поэтому log loss является стандартом для задач классификации. В следующем кейсе мы сделаем ещё один шаг и увидим, как две модели с одинаковой точностью могут иметь принципиально разное качество с точки зрения функции потерь.

---
description: Формулы, интерпретация весов.
---

# Bag of Words и TF–IDF

В предыдущих главах мы говорили о тексте как о данных и о том, что компьютер не умеет читать слова "как человек". Для него текст – это набор символов, чисел и статистик. В этой главе мы разберём два базовых, но до сих пор крайне полезных подхода к представлению текста в виде чисел: Bag of Words и TF–IDF.

Они просты, почти наивны, но именно поэтому идеально подходят для понимания того, как из слов появляется математика. Более того, многие современные идеи в NLP логически вырастают именно из них.

### Идея Bag of Words

BOW - Bag of Words (мешок слов) – это способ представить текст без учёта порядка слов. Нас интересует только то, какие слова встретились и сколько раз.

Представим два предложения:

* "Кот ест рыбу"
* "Рыбу ест кот"

Для человека они почти одинаковы. Для Bag of Words – абсолютно одинаковы.

Мы как бы высыпаем слова из текста в мешок, перемешиваем их и считаем количество каждого слова.

### Формирование словаря

Первый шаг – построить словарь. Это просто список всех уникальных слов во всех документах.

Пусть у нас есть три документа:

* D1: "кот ест рыбу"
* D2: "кот любит рыбу"
* D3: "собака ест мясо"

Словарь будет таким:

```
[кот, ест, рыбу, любит, собака, мясо]
```

Каждому слову мы сопоставляем индекс.

```
[кот→0, ест→1, рыбу→2, любит→3, собака→4, мясо→5]
```

### Вектор Bag of Words

Теперь каждый документ превращается в вектор длины $$|V|$$, где $$|V|$$ – размер словаря.

D1: "кот ест рыбу":&#x20;

```
[1, 1, 1, 0, 0, 0]
```

D2: "кот любит рыбу"

```
[1, 0, 1, 1, 0, 0]
```

D3: "собака ест мясо"

```
[0, 1, 0, 0, 1, 1]
```

Каждое число – это количество вхождений слова.

<div align="left"><figure><img src="../.gitbook/assets/19.1-bow-vectors.png" alt="" width="563"><figcaption><p>19.1 BOW - Мешок слов</p></figcaption></figure></div>

### Немного математики

Формально Bag of Words можно записать так.

Пусть:

* $$V = {w₁, w₂, …, wₙ}$$ – словарь
* $$d$$ – документ

Тогда вектор документа:

$$
x(d) = (c₁, c₂, …, cₙ)
$$

где $$cᵢ$$ – количество вхождений слова $$wᵢ$$ в документ $$d$$.

Это обычный вектор в $$ℝⁿ$$.

И уже на этом этапе мы можем:

* сравнивать документы
* обучать классификаторы
* искать похожие тексты

Но есть одна проблема.

### Проблема частот

Рассмотрим слово "кот" и слово "и".

Слово "и" будет встречаться почти в каждом документе. Его частота большая, но смысловая ценность почти нулевая.

Bag of Words не различает:

* важные слова
* служебные слова
* редкие, но информативные термины

Все слова равны. А это плохо.

И тут появляется TF–IDF.

### TF–IDF: интуиция

TF–IDF расшифровывается как: Term Frequency – Inverse Document Frequency

Идея очень простая:

* слово важно, если оно часто встречается в документе
* но оно теряет ценность, если встречается почти во всех документах

TF – "насколько важно слово в этом тексте"

IDF – "насколько слово редкое в корпусе"

Итоговый вес – их произведение.

### Term Frequency (TF)

Самый простой вариант TF:

$$
TF(w, d) = count(w, d)
$$

Но часто используют нормализацию:

$$
TF(w, d) = count(w, d) / |d|
$$

где $$|d|$$ – общее количество слов в документе.

Интерпретация:

* 0 → слова нет
* чем больше значение, тем важнее слово для конкретного документа

### Inverse Document Frequency (IDF)

IDF показывает, насколько слово редкое.

Формула:

$$
IDF(w) = log( N / df(w) )
$$

где:

* $$N$$ – общее число документов
* $$df(w)$$ – количество документов, где встречается слово w

Иногда добавляют сглаживание:

$$
IDF(w) = log( (N + 1) / (df(w) + 1) ) + 1
$$

Интерпретация:

* редкое слово → высокий IDF
* частое слово → IDF близко к 0

### Итоговая формула TF–IDF

Вес слова w в документе d:

$$
TF–IDF(w, d) = TF(w, d) × IDF(w)
$$

Таким образом:

* слово часто в документе → вес растёт
* слово часто во всех документах → вес падает

<div align="left"><figure><img src="../.gitbook/assets/19.2-tfidf-heatmap.png" alt="" width="563"><figcaption><p>19.2 Тепловая карта, отображающая значения TF-IDF</p></figcaption></figure></div>

### Пример расчёта

Пусть у нас 3 документа, и слово "кот" встречается в двух из них.

$$
N = 3 
\\\\
df(кот) = 2
$$

Итого

$$
IDF(кот) = log(3 / 2) ≈ 0.176
$$

А слово "собака" встречается только в одном документе:

<br>

$$
df(собака) = 1
$$

Поэтому

$$
IDF(собака) = log(3 / 1) ≈ 1.099
$$

Даже если в документе они встречаются по одному разу, "собака" будет весить значительно больше.

### Вектор TF–IDF

Как и Bag of Words, TF–IDF – это вектор.

Отличие только в том, что вместо целых чисел мы получаем вещественные веса.

$$
x(d) = (tfidf₁, tfidf₂, …, tfidfₙ)
$$

Этот вектор:

* разреженный
* высокоразмерный
* хорошо отражает смысл документа на базовом уровне

### Сравнение документов

TF–IDF часто используют вместе с cosine similarity.

Почему? Потому что:

* длины документов разные
* важна не сумма весов, а направление вектора

Cosine similarity измеряет угол между векторами, а не расстояние между точками.

<div align="left"><figure><img src="../.gitbook/assets/19.3-cosine-vectors.png" alt="" width="563"><figcaption><p>19.3 Косинусное сходство документов</p></figcaption></figure></div>

### Ограничения Bag of Words и TF–IDF

Важно понимать границы этих моделей.

Они:

* не учитывают порядок слов
* не понимают контекст
* не различают омонимы
* не знают семантики

"river _bank_" и "_bank_ of money" для них – почти одно и то же.

Но при этом они:

* быстрые
* интерпретируемые
* отлично работают на малых данных
* являются хорошей базовой линией

### Почему это всё ещё важно

Bag of Words и TF–IDF — это фундамент.

Если вы понимаете:

* откуда берётся вектор
* почему вес слова именно такой
* как редкость влияет на значение

то эмбеддинги, attention и трансформеры перестают быть такими непонятными. Они просто делают то же самое, но сложнее и умнее.

Именно поэтому мы начали с мешка слов.

### Простой пример TF–IDF на PHP (без библиотек)

Ниже — максимально простой пример, который показывает саму механику, без оптимизаций и абстракций. Такой код легко читать и удобно разбирать построчно.

Пусть у нас есть три документа:

```php
$documents = [
    'кот ест рыбу',
    'кот любит рыбу',
    'собака ест мясо',
];
```

#### Шаг 1. Токенизация

```php
function tokenize(string $text): array {
    return explode(' ', $text);
}

$tokenized = array_map('tokenize', $documents);
```

#### Шаг 2. Словарь

```php
$vocab = [];
foreach ($tokenized as $doc) {
    foreach ($doc as $word) {
        $vocab[$word] = true;
    }
}

$vocab = array_keys($vocab);
```

#### Шаг 3. Term Frequency (TF)

```php
function termFrequency(array $doc): array {
    $tf = [];
    $length = count($doc);

    foreach ($doc as $word) {
        $tf[$word] = ($tf[$word] ?? 0) + 1;
    }

    foreach ($tf as $word => $count) {
        $tf[$word] = $count / $length;
    }

    return $tf;
}
```

#### Шаг 4. Document Frequency и IDF

```php
function documentFrequency(array $tokenized): array {
    $df = [];

    foreach ($tokenized as $doc) {
        foreach (array_unique($doc) as $word) {
            $df[$word] = ($df[$word] ?? 0) + 1;
        }
    }

    return $df;
}

$df = documentFrequency($tokenized);
$N = count($tokenized);

$idf = [];
foreach ($df as $word => $count) {
    $idf[$word] = log($N / $count);
}
```

#### Шаг 5. TF–IDF вектор документа

```php
function tfidf(array $tf, array $idf): array {
    $vector = [];

    foreach ($tf as $word => $value) {
        $vector[$word] = $value * ($idf[$word] ?? 0);
    }

    return $vector;
}

$tfidfVectors = [];
foreach ($tokenized as $doc) {
    $tf = termFrequency($doc);
    $tfidfVectors[] = tfidf($tf, $idf);
}
```

Теперь `$tfidfVectors` содержит TF–IDF веса слов для каждого документа.

#### Пример использования и результат

Добавим простой вывод результатов, чтобы увидеть реальные числа:

```php
foreach ($tfidfVectors as $i => $vector) {
    echo "Документ " . ($i + 1) . ":" . PHP_EOL;
    foreach ($vector as $word => $value) {
        echo "  $word => " . round($value, 3) . PHP_EOL;
    }
    echo PHP_EOL;
}
```

Вывод будет примерно таким:

```
Документ 1:
  кот => 0.059
  ест => 0.059
  рыбу => 0.059

Документ 2:
  кот => 0.059
  любит => 0.366
  рыбу => 0.059

Документ 3:
  собака => 0.366
  ест => 0.059
  мясо => 0.366
```

#### Как это интерпретировать

Слова "кот", "ест" и "рыбу" встречаются в нескольких документах, поэтому их IDF маленький и итоговый вес низкий

Слова "любит", "собака" и "мясо" встречаются только в одном документе, поэтому их вес заметно выше

Даже при одинаковой частоте внутри документа редкость в корпусе сильно влияет на результат

Важно не то, что код "боевой", а то, что здесь буквально реализованы формулы из предыдущих разделов. Никакой магии — только частоты и логарифмы.

В следующей главе мы сделаем следующий шаг — поговорим о эмбеддингах как непрерывных пространствах смысла.

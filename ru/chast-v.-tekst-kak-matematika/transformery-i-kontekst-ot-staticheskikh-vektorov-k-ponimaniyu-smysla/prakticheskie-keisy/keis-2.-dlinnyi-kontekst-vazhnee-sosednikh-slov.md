# Кейс 2. Длинный контекст важнее соседних слов

Идея

Он усиливает мысль: контекст — это не только ближайшие токены

Сценарий

Два предложения:

– «Он положил ноутбук в рюкзак, потому что он был тяжёлым»

– «Он положил ноутбук в рюкзак, потому что он был маленьким»

Меняется всего одно слово, но «он» начинает относиться к разным объектам.

PHP-код

```
$sentences = [
    'Он положил ноутбук в рюкзак, потому что он был тяжёлым',
    'Он положил ноутбук в рюкзак, потому что он был маленьким',
];

$embeddings = $model->embed($sentences);

echo cosineSimilarity($embeddings[0], $embeddings[1]);
```

Что важно подчеркнуть текстом

– локально предложения почти одинаковые

– различие проявляется только на уровне всего контекста

– рекуррентным моделям здесь было бы сложнее

Вывод

> Self-attention позволяет учитывать смысловые связи через всё предложение, а не только через соседние слова.


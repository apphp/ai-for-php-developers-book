# Кейс 2. Почему евклидово расстояние хуже косинусного

Идея\
Связать теорию расстояний из предыдущих глав с embeddings.

Сценарий\
Берем те же векторы, что и в Кейсe 1.

Что делаем\
– считаем:\
– евклидово расстояние\
– косинусную близость\
– сравниваем результаты ранжирования

Вывод\
– показываем, что длина вектора и масштаб ломают евклидову метрику\
– объясняем, почему embeddings почти всегда нормализуют

Это редкий, но очень ценный кейс – он учит думать, а не просто повторять рецепты.

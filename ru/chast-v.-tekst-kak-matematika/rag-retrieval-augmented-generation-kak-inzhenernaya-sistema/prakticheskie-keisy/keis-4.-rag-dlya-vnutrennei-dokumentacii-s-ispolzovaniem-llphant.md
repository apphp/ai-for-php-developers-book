# Кейс 4. RAG для внутренней документации с использованием LLPhant

В этом кейсе мы делаем осознанный шаг от “чистого PHP” к первому специализированному PHP-фреймворку для работы с LLM – LLPhant. Это переходный момент: механика RAG уже понятна, и теперь нас интересует не изобретение инфраструктуры, а сохранение инженерного контроля при использовании готовых компонентов.

LLPhant хорош тем, что он не превращает RAG в магию. Архитектура остаётся прозрачной, а код читается как обычный PHP-проект.

#### Сценарий

В компании есть внутренняя документация:

– Markdown-файлы

– PDF-инструкции

– README из репозиториев

Сотрудники хотят задавать вопросы на естественном языке:

“Какой SLA у сервиса X?”

“Как часто происходит ротация API-ключей?”

Ключевое требование – модель должна отвечать строго на основе документов. Если ответа в базе нет, она должна честно сообщить об этом.

#### Почему LLPhant подходит для этого кейса

LLPhant закрывает сразу несколько инфраструктурных задач:

– генерация эмбеддингов

– хранение векторных представлений

– similarity search

При этом:

– контекст мы собираем сами

– prompt остаётся под нашим контролем

– никакой скрытой логики принятия решений нет

Для учебного кейса это идеальный баланс между ручной реализацией и фреймворком.

#### Архитектура пайплайна

\[PLACEHOLDER: схема RAG с LLPhant – Documents → Chunking → Embeddings → Vector Store → Context → LLM]

Пайплайн выглядит так:

1. Документы разбиваются на чанки
2. Для чанков считаются эмбеддинги
3. Эмбеддинги сохраняются во векторное хранилище
4. По пользовательскому запросу извлекаются Top-K фрагментов
5. Из них формируется контролируемый контекст
6. LLM генерирует ответ строго на его основе

#### Установка

```
composer require llphant/llphant
```

#### Индексация документов

```
use LLPhant\Embeddings\OpenAIEmbeddingGenerator;
use LLPhant\VectorStores\Memory\MemoryVectorStore;
use LLPhant\Document;

$embeddingGenerator = new OpenAIEmbeddingGenerator(
    apiKey: getenv('OPENAI_API_KEY')
);

$vectorStore = new MemoryVectorStore($embeddingGenerator);

$documents = [
    new Document('SLA сервиса X составляет 99.9% в месяц.'),
    new Document('Ротация API-ключей происходит каждые 90 дней.'),
];

$vectorStore->addDocuments($documents);
```

На этом этапе LLPhant берёт на себя расчёт эмбеддингов и их хранение, но не вмешивается в архитектуру RAG как системы.

<br>

#### Поиск релевантных фрагментов

```
$query = 'Как часто происходит ротация API-ключей?';

$relevantDocs = $vectorStore->similaritySearch($query, 2);
```

Важно: мы получаем не ответ, а строительный материал для него – релевантные куски документации.

#### Context building: контроль вместо магии

```
$context = "Ты отвечаешь строго на основе контекста ниже. ";
$context .= "Если ответа нет – скажи, что информации недостаточно.\n\n";
$context .= "Контекст:\n";

foreach ($relevantDocs as $doc) {
    $context .= "- " . $doc->content . "\n";
}

$context .= "\nВопрос: {$query}";
```

Даже при использовании фреймворка именно здесь RAG становится инженерной системой, а не просто вызовом LLM.

#### Генерация ответа

```
use LLPhant\Chat\OpenAIChat;
use LLPhant\Chat\Message;

$chat = new OpenAIChat(apiKey: getenv('OPENAI_API_KEY'));

$response = $chat->generate([
    Message::system($context)
]);

echo $response->content;
```

#### Выводы по кейсу

Этот кейс показывает важный принцип:

LLPhant не делает RAG за нас, он:

– убирает инфраструктурный шум

– ускоряет разработку

– сохраняет контроль над логикой и контекстом

Именно поэтому он отлично подходит как первый фреймворк


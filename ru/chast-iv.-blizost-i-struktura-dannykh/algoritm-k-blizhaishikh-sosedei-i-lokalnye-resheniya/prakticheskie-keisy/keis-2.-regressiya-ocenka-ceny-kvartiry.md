# Кейс 2. Регрессия: оценка цены квартиры

Во втором кейсе мы рассмотрим использование алгоритма k-ближайших соседей не для классификации, а для регрессии. Если в предыдущем примере результатом был класс пользователя, то теперь результатом будет числовое значение – оценка стоимости квартиры.

Этот пример особенно важен, потому что он показывает: kNN – это не "алгоритм для голосования", а универсальный механизм локального решения. Разница между классификацией и регрессией заключается лишь в способе агрегации соседей.

#### Цель кейса

Цель этого кейса – показать:

* как kNN применяется к числовым целевым значениям
* как работает локальное усреднение
* почему kNN можно интерпретировать как простейший [нелинейный регрессор](../../../vvedenie/glossarii.md#nelineinyi-regressor)
* как геометрия пространства признаков напрямую влияет на предсказанную цену

Мы сознательно используем минимальный набор признаков, чтобы сохранить прозрачность вычислений.

#### Сценарий

Представим, что мы хотим оценить стоимость квартиры на основе двух факторов:

* площадь (в квадратных метрах)
* расстояние до центра города (в километрах)

Интуитивно понятно, что:

* большая площадь увеличивает цену
* большая удалённость от центра обычно её снижает

В реальной жизни модель ценообразования гораздо сложнее, но для демонстрации принципа нам достаточно двух признаков.

В этом кейсе kNN используется как [локальный регрессор](../../../vvedenie/glossarii.md#lokalnyi-regressor): цена новой квартиры определяется на основе цен ближайших по характеристикам объектов.

#### Данные

Небольшой обучающий набор:

```php
$dataset = [
    [[40, 12], 120000],
    [[50, 10], 150000],
    [[60, 8], 190000],
    [[70, 6], 240000],
    [[80, 5], 300000],
];

$query = [65, 7];
$k = 3;
```

Каждый элемент массива содержит:

* вектор признаков \[area, distanceToCenter]
* фактическую цену квартиры

Точка `$query = [65, 7]` – это новая квартира площадью 65 м² на расстоянии 7 км от центра.

Мы будем учитывать трёх ближайших соседей.

#### Геометрическая интерпретация

Каждая квартира – это точка на плоскости признаков.

Ось X – площадь.

Ось Y – расстояние до центра.

Цена при этом не участвует в вычислении расстояния – она используется только после того, как найдены ближайшие соседи.

Таким образом, алгоритм работает в два этапа:

1. Геометрический – поиск ближайших точек.
2. Числовой – усреднение их цен.

#### Предсказание

Предполагается, что функция `euclideanDistance()` уже определена (как в предыдущем кейсе).

```php
$distances = [];

foreach ($dataset as [$features, $price]) {
    $distances[] = [
        'distance' => euclideanDistance($features, $query),
        'price' => $price
    ];
}

usort($distances, fn($a, $b) => $a['distance'] <=> $b['distance']);
$neighbors = array_slice($distances, 0, $k);

$sum = 0;
foreach ($neighbors as $neighbor) {
    $sum += $neighbor['price'];
}

$prediction = $sum / $k;

echo "Estimated price: $prediction";

// Результат
// Estimated price: 243333
```

Алгоритм выполняет следующие шаги:

* считает расстояние от новой квартиры до каждой известной
* сортирует их по близости
* выбирает три ближайших
* усредняет их цену

Именно это среднее значение и становится прогнозом.

#### Интуиция: локальное усреднение

Главная идея здесь проста: цена новой квартиры – это средняя цена квартир, которые находятся рядом с ней в пространстве признаков.

Важно подчеркнуть: мы не строим формулу вида

$$
price = a * area + b * distance + c
$$

kNN не ищет коэффициенты и не предполагает линейную зависимость.

Он говорит иначе:&#x20;

> Покажи мне похожие квартиры – и я скажу тебе примерную цену.

Это делает отображение "признаки → предсказание" нелинейным. В разных областях пространства признаков поведение может быть разным, потому что решение всегда принимается локально.

<details>

<summary>Кейс 1. Полный пример кода на чистом PHP</summary>

```php
function euclideanDistance(array $a, array $b): float {
    $sum = 0.0;

    foreach ($a as $i => $value) {
        $sum += ($value - $b[$i]) ** 2;
    }

    return sqrt($sum);
}

$dataset = [
    [[40, 12], 120000],
    [[50, 10], 150000],
    [[60, 8], 190000],
    [[70, 6], 240000],
    [[80, 5], 300000],
];

$query = [65, 7];
$k = 3;

$distances = [];

foreach ($dataset as [$features, $price]) {
    $distances[] = [
        'distance' => euclideanDistance($features, $query),
        'price' => $price
    ];
}

usort($distances, fn($a, $b) => $a['distance'] <=> $b['distance']);
$neighbors = array_slice($distances, 0, $k);

$sum = 0;
foreach ($neighbors as $neighbor) {
    $sum += $neighbor['price'];
}

$prediction = $sum / $k;

echo "Estimated price: $prediction";
```

</details>

#### Почему это нелинейная модель

Если бы зависимость цены от площади и расстояния была строго линейной, можно было бы использовать линейную регрессию.

Однако kNN способен описывать сложные формы зависимости, потому что:

* в одной области пространства соседями будут одни объекты
* в другой области – совершенно другие
* функция предсказания фактически "собирается" из кусочков

Таким образом, kNN реализует кусочно‑локальную аппроксимацию функции.

#### Ограничения метода

Простота алгоритма может создать ощущение универсальности, однако у него есть важные ограничения:

* чувствительность к масштабу признаков (нужна нормализация)
* высокая вычислительная стоимость при больших объёмах данных
* ухудшение работы при росте размерности

В нашем примере всё выглядит аккуратно только потому, что пространство двумерное и набор данных маленький.&#x20;

Кроме того, в данном примере масштабы признаков сопоставимы (площадь и расстояние имеют близкие численные диапазоны), поэтому влияние масштабирования почти незаметно. В реальных задачах нормализация признаков критически важна.

#### Выводы

Этот кейс демонстрирует, что kNN:

* одинаково естественно работает и для классификации, и для регрессии
* реализует идею локального усреднения
* не требует обучения с подбором параметров модели – обучение сводится к сохранению обучающих данных
* напрямую связывает предсказание с геометрией данных

Если в предыдущем кейсе соседи голосовали, то здесь они "складываются и делятся". Но философия алгоритма остаётся неизменной – решение принимается в локальной окрестности точки.

Понимание этого примера формирует интуицию о том, что машинное обучение – это не всегда поиск глобальной формулы. Иногда это просто аккуратная работа с соседством и расстояниями.

{% hint style="info" %}
Чтобы самостоятельно протестировать этот код, установите примеры из официального репозитория [GitHub](https://github.com/apphp/ai-for-php-developers-examples) или воспользуйтесь [онлайн-демонстрацией](https://aiwithphp.org/books/ai-for-php-developers/examples/part-4/k-nearest-neighbors-algorithm-and-local-solutions) для его запуска.
{% endhint %}

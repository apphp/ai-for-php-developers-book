---
description: Геометрическая интуиция, метрики расстояний.
---

# Алгоритм k-ближайших соседей и локальные решения

Алгоритм k-ближайших соседей (k-Nearest Neighbors, k-NN) – один из самых интуитивных и при этом фундаментальных алгоритмов машинного обучения. Он почти не делает предположений о данных, не обучает параметрическую модель, а хранит обучающие данные и опирается на простую, почти геометрическую идею: _похожие объекты должны иметь похожие ответы_.

Именно поэтому k-NN особенно хорошо подходит для объяснения того, что такое локальные решения, почему геометрия данных важна и как выбор метрики расстояния напрямую влияет на результат.

### Локальные решения вместо глобальной модели

В отличие от [линейной](../../vvedenie/glossarii.md#lineinaya-regressiya) или [логистической](../../vvedenie/glossarii.md#logisticheskaya-regressiya) регрессии, k-NN не пытается найти одну общую формулу для всех данных. У него нет коэффициентов, весов или оптимизируемой функции потерь в явном параметрическом виде.

Алгоритм работает иначе:

1. Мы сохраняем все обучающие данные.
2. Для нового объекта ищем _k_ ближайших к нему примеров.
3. Принимаем решение, глядя только на этих соседей.

Это и есть локальное решение. Для каждой новой точки решение строится заново, исходя из её ближайшего окружения.

Можно сказать, что k-NN каждый раз отвечает на вопрос:

> Что обычно происходит с объектами, _похожими именно на этот_?

<div align="left"><figure><img src="../../.gitbook/assets/16.1-knn-local-decision.png" alt="" width="563"><figcaption><p>16.1 Локальные решения c k-NN</p></figcaption></figure></div>

### Геометрическая интуиция k-NN

Рассмотрим самый простой случай – пространство из двух признаков. Каждый объект – это точка на плоскости. Тогда работа алгоритма выглядит буквально геометрически:

* есть точка запроса
* мы измеряем расстояния до всех остальных точек
* выбираем $$k$$ точек с минимальным расстоянием

Для классификации чаще всего используется голосование:

$$
\hat{y} = \operatorname{mode}(y_1, y_2, \dots, y_k)
$$

Для регрессии – усреднение:

$$
\hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i
$$

Таким образом, решение определяется формой локального "облака" точек вокруг запроса.

### Метрики расстояния: как мы определяем "близость"

Ключевой вопрос k-NN – что значит "ближайший"? Ответ задаётся метрикой расстояния.

#### Евклидово расстояние

[Евклидово расстояние](../../vvedenie/glossarii.md#evklidovo-rasstoyanie) - самая распространённая метрика:

$$
d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
$$

Она хорошо работает, когда:

* признаки имеют одинаковый масштаб
* пространство относительно низкой размерности (см. проклятие размерности ниже)
* важна геометрическая форма облаков

<div align="left"><figure><img src="../../.gitbook/assets/16.2-euclidean-distance.png" alt="" width="563"><figcaption><p>16.2 Евклидово расстояние</p></figcaption></figure></div>

#### Манхэттенское расстояние

[Манхэттенское расстояние](../../vvedenie/glossarii.md#mankhettenskoe-rasstoyanie) - это когда важнее не прямая линия, а сумма перемещений по осям:

$$
d(x, y) = \sum_{i=1}^{n} |x_i - y_i|
$$

Эта метрика часто используется, когда вклад каждого признака в расстояние аддитивен и имеет интерпретацию "стоимости шага".

<div align="left"><figure><img src="../../.gitbook/assets/16.3-manhattan-distance.png" alt="" width="563"><figcaption><p>16.3 Манхэттенское расстояние</p></figcaption></figure></div>

#### Расстояние Минковского

Обобщающая форма для [расстояния Минковского](../../vvedenie/glossarii.md#minkovskogo-rasstoyanie):

$$
d(x, y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{1/p}
$$

* при $$p = 2$$ получаем евклидово расстояние
* при $$p = 1$$ – манхэттенское

Выбор p позволяет плавно менять форму "окрестности" точки.

<div align="left"><figure><img src="../../.gitbook/assets/16.4-minkowski_distance.png" alt="" width="563"><figcaption><p>16.4 Расстояние Минковского</p></figcaption></figure></div>

#### Косинусное расстояние

Когда важна не длина вектора, а направление. Формально это [косинусное сходство](../../vvedenie/glossarii.md#cosine-similarity-kosinusnoe-skhodstvo), а не расстояние.

$$
\cos(\theta) = \frac{x \cdot y}{||x|| \cdot ||y||}
$$

На практике в k-NN обычно используют косинусное расстояние:

$$
d(x, y) = 1 - \cos(\theta)
$$

Строго говоря, эта функция не является метрикой в математическом смысле, но широко используется на практике. Например: используется в задачах с текстами, эмбеддингами, рекомендациями, где абсолютные значения менее важны, чем относительные пропорции.

<div align="left"><figure><img src="../../.gitbook/assets/16.5-cosine-distance.png" alt="" width="563"><figcaption><p>16.5 Косинусное расстояние</p></figcaption></figure></div>

### Выбор k, компромисс смещения и дисперсии

Параметр $$k$$ определяет, насколько локальным будет решение.

Малое k (например, $$k = 1$$):

* очень чувствителен к шуму
* низкое смещение, высокая дисперсия

Большое k:

* решения более сглаженные
* выше смещение, ниже дисперсия

<div align="left"><figure><img src="../../.gitbook/assets/16.6-knn-bias-variance.png" alt="" width="563"><figcaption><p>16.6 Сравнение границ принятия решений k-NN</p></figcaption></figure></div>

k-NN – наглядный пример классического компромисса [bias–variance](../../vvedenie/glossarii.md#bias-variance-tradeoff).

### Размерность и проклятие размерности

С ростом числа признаков возникает проблема, известная как проклятие размерности.

Интуитивно:

* в высоких размерностях все точки становятся "далёкими"
* различия между ближайшим и дальним соседом уменьшаются
* метрики расстояния теряют выразительность

Для многих распределений в высоких размерностях наблюдается эффект, при котором отношение:

$$
\frac{d_{\min}}{d_{\max}} \to 1
$$

Это делает k-NN менее эффективным без:

* нормализации признаков
* отбора признаков
* снижения размерности (PCA, autoencoders)

### Почему k-NN важен концептуально

Несмотря на простоту и вычислительную дороговизну при прямом поиске на больших данных, k-NN остаётся ключевым алгоритмом для понимания машинного обучения:

* он показывает разницу между локальными и глобальными моделями
* делает геометрию данных зримой
* подчёркивает роль метрик и масштабов
* демонстрирует компромисс bias–variance без сложной математики

k-NN – это почти "честный" алгоритм. Он не прячет логику за весами и слоями, а напрямую говорит: _посмотри на своих соседей_.

Именно поэтому он так хорошо подходит для обучения интуиции, даже если в промышленном коде его используют не так часто.

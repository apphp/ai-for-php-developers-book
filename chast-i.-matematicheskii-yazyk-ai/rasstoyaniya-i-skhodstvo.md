---
description: >-
  Евклидово расстояние, dot product, cosine similarity. Кейс: сравнение текстов
  и объектов.
---

# Расстояния и сходство

### Расстояния и сходство

Как только мы представили данные в виде чисел и векторов, возникает естественный вопрос: как понять, что два объекта похожи или, наоборот, сильно отличаются? Машинное обучение почти всегда сводится к сравнению. Этот текст ближе к тому или к этому, этот пользователь похож на другого или нет, это изображение относится к классу A или B.

Чтобы формализовать такие рассуждения, нам нужны меры расстояния и меры сходства. В математике и ML это не абстрактные термины, а конкретные функции, которые принимают два вектора и возвращают число. По этому числу алгоритм принимает решения.

В этой главе мы разберем три ключевых инструмента: евклидово расстояние, скалярное произведение (dot product) и косинусное сходство. Они лежат в основе kNN, линейных моделей, рекомендательных систем, поиска по текстам и эмбеддингов.

### Евклидово расстояние – “обычная” геометрия

\
Евклидово расстояние – это расстояние “по линейке” между двумя точками в пространстве.

Если у нас есть два вектора

$$
x = (x_1, x_2, \dots, x_n) \\ 
y = (y_1, y_2, \dots, y_n)
$$

то евклидово расстояние между ними определяется формулой:

$$
d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
$$

Если $$n = 2$$, это расстояние между точками на плоскости. Если $$n = 100$$ или, например, $$768$$, то геометрия остается той же самой, просто ее уже нельзя нарисовать напрямую.

<div align="left"><figure><img src="../.gitbook/assets/7.1-euclidean-distance.png" alt="" width="563"><figcaption><p>Евклидово расстояние в 2D</p></figcaption></figure></div>

На плоскости это выглядит очень наглядно: две точки и отрезок между ними. В машинном обучении мы делаем ровно то же самое, только в пространстве большей размерности.

Кам вы наверняка запомнили из предыдущей главы – евклидово расстояние чувствительно к масштабу и поэтому перед использованием такого расстояния данные почти всегда нормализуют.

Интуитивно: два объекта похожи, если расстояние между их векторами маленькое.

**PHP-пример**

```php
function euclideanDistance(array $a, array $b): float {
    $sum = 0.0;
    foreach ($a as $i => $value) {
        $diff = $value - $b[$i];
        $sum += $diff ** 2;
    }
    return sqrt($sum);
}
```

Этот код уже можно использовать в простейшем kNN-классификаторе: мы просто ищем объекты с минимальным расстоянием.

\>>>>>>>>>

#### Скалярное произведение (dot product) – мера согласованности

Скалярное произведение двух векторов определяется как:

$$
x \cdot y = \sum_{i=1}^{n} x_i y_i
$$

На первый взгляд это просто сумма произведений. Но геометрически справедливо:

$$
x \cdot y = |x| |y| \cos(\theta)
$$

где $$\theta$$ – угол между векторами, а длина вектора:

$$
|x| = \sqrt{x \cdot x}
$$

\[Иллюстрация 2: Два вектора и угол между ними]

Dot product становится большим, когда:

* векторы направлены примерно в одну сторону;
* у них большие длины.

Это ключевой момент: скалярное произведение учитывает и направление, и масштаб. Поэтому оно не является “чистым” сходством.

В машинном обучении это используется осознанно. В линейных моделях большой dot product означает сильную активацию. В нейросетях и attention-механизмах он интерпретируется как мера важности или связи.

\[Иллюстрация 3: Dot product как проекция одного вектора на другой]

Если представить $$x \cdot y$$ геометрически, то это похоже на проекцию одного вектора на направление другого, умноженную на длину.

**PHP-пример**

```php
function dotProduct(array $a, array $b): float {
    $sum = 0.0;
    foreach ($a as $i => $value) {
        $sum += $value * $b[$i];
    }
    return $sum;
}
```

#### Косинусное сходство – сравнение направлений

Как мы уже упоминали в прошлой главе - косинусное сходство отвечает на вопрос: насколько векторы направлены в одну сторону, независимо от их длины.

Оно определяется формулой:

$$
\text{cosine_sim}(x, y) = \frac{x \cdot y}{|x| |y|}
$$

Результат лежит в диапазоне от -1 до 1:

* 1 – одинаковое направление;
* 0 – [ортогональные векторы](#user-content-fn-1)[^1];
* -1 – противоположные направления.

\[Иллюстрация 4: Один и тот же угол, разные длины векторов]

На этой картинке важно увидеть: длины разные, но угол одинаковый – значит, косинусное сходство одинаковое.

Это делает cosine similarity идеальной мерой для текстов и эмбеддингов, где длина вектора часто отражает не смысл, а масштаб (длина текста, частота слов).

**PHP-пример**

```php
function cosineSimilarity(array $a, array $b): float {
    $dot = dotProduct($a, $b);
    $normA = sqrt(dotProduct($a, $a));
    $normB = sqrt(dotProduct($b, $b));
    return $dot / ($normA * $normB);
}
```

#### Кейс 1: сравнение текстов

Предположим, мы представили тексты в виде векторов – через bag-of-words, TF-IDF или эмбеддинги языковой модели. Каждый текст теперь – точка в пространстве.

\[Иллюстрация 5: Тексты как точки в векторном пространстве]

Если использовать евклидово расстояние, длинные тексты часто оказываются дальше просто потому, что в них больше слов.

Косинусное сходство сравнивает не длину, а направление вектора – распределение смыслов. Два текста про одно и то же, но разной длины, будут близки.

Поэтому поиск по документам, кластеризация статей и семантический поиск почти всегда используют cosine similarity.

Алгоритм при этом очень простой:

1. Преобразовать текст в вектор.
2. Посчитать косинусное сходство с другими.
3. Отсортировать по убыванию сходства.

#### Кейс 2: сравнение объектов и пользователей

Для числовых объектов ситуация другая. Если признаки имеют одинаковый смысл и масштаб, евклидово расстояние часто оказывается более естественным.

Например, пользователь описывается вектором:

$$
(\text{возраст}, \text{доход}, \text{частота покупок})
$$

После нормализации евклидово расстояние отражает “общую близость” профилей.

Dot product часто используется в рекомендательных системах. Большое значение означает: пользователь и объект хорошо сочетаются, причем учитывается сила предпочтений.

\[Иллюстрация 6: Пользователи и товары в одном пространстве]

#### Как выбрать меру расстояния

Выбор расстояния – это способ формализовать ваше представление о похожести.

Если важна геометрическая близость и масштаб признаков – евклидово расстояние.

Если важна сила взаимодействия и вклад признаков – dot product.

Если важен смысл и направление, особенно в текстах – cosine similarity.

Алгоритм машинного обучения не знает, что такое “похоже”. Он знает только числа, которые вы ему даете. Мера расстояния – это ваш перевод интуиции на язык математики.

[^1]: Ортогональные векторы — это векторы, скалярное произведение которых равно нулю, что геометрически означает их взаимную перпендикулярность (угол ![](data:image/gif;base64,R0lGODlhAQABAIAAAP///wAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==)90 градусов между ними).

---
description: Почему данные – это точки в пространстве.
---

# Векторы, размерности и пространства признаков

### Векторы, размерности и пространства признаков

Если убрать из машинного обучения все сложные слова и модные термины, то в сухом остатке почти всегда останется одна и та же идея: мы представляем реальные объекты числами и работаем с этими числами математически. Именно здесь на сцену выходят векторы, размерности и пространства признаков. Для PHP-разработчика особенно важно понять это интуитивно, а не формально, потому что в коде вы постоянно будете иметь дело не с абстрактной линейной алгеброй, а с массивами чисел, матрицами и операциями над ними.

#### **Вектор как способ описать объект**

Вектор в контексте машинного обучения – это просто упорядоченный набор чисел. Каждое число описывает какой-то аспект объекта. Если объект простой, вектор короткий. Если объект сложный, вектор может быть очень длинным.

Представим пользователя интернет-магазина. Мы можем описать его так: возраст, количество покупок за год, средний чек. Тогда один пользователь – это вектор из трех чисел:

(возраст, покупки, средний чек)

В PHP это будет выглядеть максимально приземленно:

```php
$userVector = [35, 12, 78.5];
```

Важно понять: вектор – это не просто массив. Порядок элементов имеет смысл. Если вы перепутаете местами возраст и средний чек, модель не «догадается», что вы имели в виду. Для нее это будут другие данные.

#### **Размерность вектора**

Размерность вектора – это количество чисел в нем. В примере выше размерность равна 3. Если вы добавите еще один признак, например, “дней с последней покупки”, размерность станет 4.

Размерность напрямую связана с тем, насколько подробно вы описываете объект. Низкая размерность означает грубое описание, высокая – более детальное. Но высокая размерность не всегда лучше. Каждый дополнительный признак – это новая степень свободы для модели и новый источник шума.

Для PHP-разработчика полезно думать о размерности как о фиксированном контракте. Если модель ожидает вектор длины 10, вы обязаны всегда передавать ровно 10 чисел, в одном и том же порядке.

Формально пространство признаков обычно обозначают как $$R^n$$. Это означает, что каждый объект описывается вектором из $$n$$ вещественных чисел, а все такие векторы образуют единое абстрактное пространство. В нём определены операции сложения векторов и умножения на число, что позволяет применять инструменты линейной алгебры. В практическом машинном обучении нам не так важны формальные аксиомы, но важно понимать, что модель всегда работает внутри фиксированного $$n$$-мерного пространства, заданного структурой признаков.

```php
function predict(array $features): float {
    if (count($features) !== 10) {
        throw new InvalidArgumentException("Ожидается вектор размерности 10");
    }

    // дальнейшие вычисления
}
```

#### **Пространство признаков**

Когда мы говорим не об одном векторе, а о всех возможных векторах данной размерности, мы попадаем в пространство признаков. Формально, если у нас есть вектор признаков$$x = (x₁, x₂, …, xₙ)$$, то он является элементом $$n$$-мерного вещественного пространства $$R^n$$. Это не абстрактная формула ради формулы – это точное математическое описание того, что делает большинство моделей машинного обучения.

Если размерность равна 2, пространство признаков – это обычная плоскость. Если 3 – привычное трехмерное пространство. Если больше, то визуализировать его уже невозможно, но математически это то же самое пространство $$R^n$$.

Каждая точка в этом пространстве – это один объект из реального мира, переведенный на язык чисел. Пользователь, товар, документ, изображение – все они после подготовки данных становятся точками в пространстве признаков.

Хорошей визуализацией для PHP-разработчика может быть такое: пространство признаков – это то, где живут ваши данные после этапа [feature engineering](#user-content-fn-1)[^1]. До этого у вас были строки, даты, категории и JSON. После – только числа.

#### **Признаки как оси координат**

Каждый признак – это отдельная ось координат в пространстве. Математически это означает, что значение признака $$xᵢ$$ – это координата точки вдоль $$i$$-й оси.

Например, вектор (35, 12, 78.5) – это точка в трехмерном пространстве, где первая ось – возраст, вторая – количество покупок, третья – средний чек.

Это сразу объясняет несколько важных вещей.

Во-первых, признаки должны быть сопоставимы по масштабу (ещё одна причина, почему мы используем преобразование признаков в числа). Если одна ось измеряется в десятках, а другая – в сотнях тысяч, то геометрия пространства искажается. Расстояния и углы между точками перестают отражать реальное сходство объектов.

Во-вторых, добавление нового признака – это добавление новой оси. Пространство становится более размерным (плюс ещё один размер), а каждая точка получает еще одну координату. Модель вынуждена учитывать еще одно направление при принятии решений.

### **Нормализация и масштабирование**

На практике почти всегда приходится приводить признаки к одному масштабу. Это можно сделать разными способами: нормализацией (приведение значений к фиксированному диапазону, например от 0 до 1) или стандартизацией (приведение распределения к нулевому среднему и единичному стандартному отклонению). Оба подхода решают одну инженерную задачу — сделать признаки сопоставимыми для алгоритма.\
\
Это не прихоть математиков, а чисто инженерная необходимость, так как большинство алгоритмов машинного обучения чувствительны к масштабу входных данных: признаки с большими числовыми значениями начинают доминировать над остальными, искажая вклад действительно информативных признаков. Тем самым ухудшается качество и стабильность обучения модели.

#### **Нормализация**

Простейший пример – нормализация в диапазон от 0 до 1:

```php
function normalize(float $value, float $min, float $max): float {
    if ($max === $min) {
        return 0.0;
    }
    return ($value - $min) / ($max - $min);
}
```

После такой обработки возраст, количество покупок и средний чек начинают “весить” примерно одинаково в пространстве признаков.

#### **Стандартизация**

Другой широко используемый подход – стандартизация признаков. В отличие от нормализации, она не ограничивает значения фиксированным диапазоном, а приводит распределение признака к виду с нулевым средним и единичным стандартным отклонением. Это особенно важно для моделей, которые предполагают данные, близкие к нормальному распределению (линейная регрессия, логистическая регрессия, SVM, нейронные сети). Стандартизация делает признаки сопоставимыми по масштабу, но при этом сохраняет информацию о выбросах и относительных отклонениях значений.

Простейшая формула стандартизации выглядит так:

```php
function standardize(float $value, float $mean, float $std): float {
    if ($std == 0.0) {
        return 0.0;
    }
    return ($value - $mean) / $std;
}
```

После стандартизации возраст, количество покупок и средний чек имеют среднее значение около нуля и сопоставимую дисперсию, благодаря чему модель обучается стабильнее, а процесс оптимизации становится быстрее и предсказуемее.

### **Категориальные признаки и размерность**

Не все признаки изначально числовые. Цвет, страна, тип устройства – все это категории. Чтобы поместить их в пространство признаков, их нужно превратить в числа. Чаще всего это делается через [one-hot encoding](#user-content-fn-2)[^2].

Если у нас есть три возможных цвета: red, green, blue, то один признак превращается в три координаты:

```php
function encodeColor(string $color): array {
    return [
        $color === 'red' ? 1 : 0,
        $color === 'green' ? 1 : 0,
        $color === 'blue' ? 1 : 0,
    ];
}
// Result:
// Red:    [1, 0, 0]
// Green:  [0, 1, 0]
// Blue:   [0, 0, 1]
```

Здесь важно заметить, что размерность резко растет. Один логический признак превратился в три числовых. В реальных задачах с сотнями категорий это становится серьезной проблемой и напрямую влияет на сложность моделей.

#### **Почему данные – это точки в пространстве**

Причина, по которой данные в машинном обучении рассматриваются как точки в пространстве, проста и фундаментальна. Большинство алгоритмов опираются на геометрию: расстояния, углы, проекции и поверхности.

Если у вас есть набор объектов, каждый из которых описан одним и тем же набором числовых признаков, то строгий математический способ работать с ними – рассматривать их как точки в одном и том же пространстве $$R^n$$ .

Формально: пусть каждый объект описывается вектором $$x = (x₁, x₂, …, xₙ)$$. Тогда вся выборка – это конечное множество точек $${x¹, x², …, xᵐ} ⊂ Rⁿ$$.

Расстояние между двумя точками $$||x − y||$$ отражает степень их сходства. Направление вектора $$(x − y)$$ показывает, в каких признаках объекты отличаются сильнее всего. Плоскость или гиперплоскость – это множество точек, удовлетворяющих линейному уравнению.

Именно поэтому даже самые разные модели в итоге сводятся к геометрическим операциям над векторами.

<div align="left"><figure><img src="../.gitbook/assets/6.1-points-in-2d-feature-space.png" alt="" width="563"><figcaption><p>Точки в 2D-пространстве признаков</p></figcaption></figure></div>

#### **Геометрический смысл расстояний и углов**

В пространстве признаков важны не только расстояния, но и углы между векторами. Угол между векторами показывает, насколько два направления изменений похожи. Формально речь идёт о направленном сходстве: даже если значения сильно различаются по масштабу, малый угол означает, что признаки изменяются согласованно. Именно поэтому косинусное сходство часто используется при работе с текстовыми эмбеддингами и высокоразмерными данными.

Скалярное произведение двух векторов $$x$$ и $$y$$ определяется как:

$$
x · y = Σᵢ xᵢ yᵢ
$$

Через него выражается косинус угла между векторами:

$$
cos(θ) = (x · y) / (||x|| · ||y||)
$$

Это напрямую используется, например, при сравнении текстовых эмбеддингов или пользовательских профилей.

<div align="left"><figure><img src="../.gitbook/assets/6.2-angle-between-2-vectors.png" alt="" width="563"><figcaption><p>Угол между двумя векторами</p></figcaption></figure></div>

Для PHP-разработчика это означает простую вещь: модель может считать два объекта похожими не потому, что они близки по всем координатам, а потому, что они “смотрят” в одном направлении в пространстве признаков.

#### **Связь с конкретными алгоритмами**

Алгоритм k ближайших соседей (kNN - k-Nearest Neighbors) буквально живет в пространстве признаков. Он ничего не обучает в классическом смысле, а просто для новой точки ищет k ближайших точек по выбранной метрике расстояния.

Математически это выглядит так: для нового вектора x мы ищем такие векторы $$xᵢ$$ из обучающей выборки, для которых расстояние $$d(x, xᵢ)$$ минимально.

<div align="left"><figure><img src="../.gitbook/assets/6.3.-knn-in-feature-space.png" alt="" width="563"><figcaption><p>kNN в пространстве признаков</p></figcaption></figure></div>

Функция указанная ниже реализует классическую формулу евклидова расстояния между двумя точками:

$$
d(a, b) = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}
$$

```php
function euclideanDistance(array $a, array $b): float {
    $sum = 0.0;
    foreach ($a as $i => $value) {
        $diff = $value - $b[$i];
        $sum += $diff * $diff;
    }
    return sqrt($sum);
}
```

> Евклидово расстояние — самый интуитивный и часто используемый способ измерять расстояние между точками в пространстве признаков, но оно не является универсальным. В зависимости от задачи и природы данных могут применяться другие метрики: например, манхэттенское расстояние или косинусная мера сходства. Разные метрики по-разному определяют понятие «близости» между объектами, и выбор метрики напрямую влияет на поведение алгоритма и результаты модели.

Линейные модели смотрят на пространство иначе. Они ищут гиперплоскость, которая лучше всего разделяет точки или аппроксимирует их значения.

Формально линейная модель записывается так:

$$
f(x) = \mathbf{w} \cdot \mathbf{x} + b
$$

Геометрически это означает, что все точки, для которых

$$
\mathbf{w} \cdot \mathbf{x} + b = 0
$$

лежат на одной гиперплоскости. Знак $$f(x)$$ определяет, по какую сторону границы находится точка.&#x20;

Если подобрать более «инженерную» формулировку, то можно написать так:

> Линейная модель разбивает пространство признаков гиперплоскостью, а знак линейной функции определяет класс объекта.

<div align="left"><figure><img src="../.gitbook/assets/6.4-linear-decision-boundary.png" alt="" width="563"><figcaption><p>Линейная граница принятия решений</p></figcaption></figure></div>

Функция ниже вычисляет:

$$
f(x) = \mathbf{w} \cdot \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b
$$

Геометрический смысл

* Значение функции показывает, насколько далеко точка находится от разделяющей гиперплоскости (с точностью до масштаба).
* Если $$f(x) = 0$$ – точка лежит на гиперплоскости.
* Если $$f(x) > 0$$ или $$f(x) < 0$$ – по разные стороны границы.

```php
function linearModel(array $x, array $w, float $b): float {
    $sum = $b;
    foreach ($x as $i => $value) {
        $sum += $value * $w[$i];
    }
    return $sum;
}
```

Нейронные сети – это следующий шаг усложнения. Каждый слой выполняет [аффинное преобразование](#user-content-fn-3)[^3]:

$$
z = W x + b
$$

А затем применяет нелинейную функцию активации. Геометрически это означает, что пространство сначала линейно поворачивается и растягивается, а затем нелинейно “ломается”.

После нескольких таких преобразований данные, которые были неразделимы линейно, становятся разделимыми.

<div align="left"><figure><img src="../.gitbook/assets/6.5-nonlinear-transformation-of-space.png" alt="Нелинейное преобразование пространства" width="563"><figcaption></figcaption></figure></div>



Несмотря на сложность архитектур, вход у нейросети всегда один и тот же – вектор фиксированной размерности.

### **Высокая размерность и ее последствия**

Когда размерность пространства признаков становится большой, возникают эффекты, которые на интуитивном уровне кажутся странными. Объем пространства растет экспоненциально, точки становятся разреженными, а расстояния между ними выравниваются. Это часто называют “проклятием размерности”.

Для PHP-разработчика практический вывод простой: не добавляйте признаки “на всякий случай”. Каждый признак должен иметь понятный смысл и пользу для задачи.

#### **Связь с реальными моделями**

Линейная регрессия, логистическая регрессия, нейронные сети – все они работают в пространстве признаков. Разница лишь в том, какие поверхности они могут в нем строить. Линейная модель проводит плоскость или гиперплоскость. Нейросеть – сложную нелинейную форму.

Но вход у них у всех один и тот же – вектор фиксированной размерности.

```php
$features = [0.42, 0.15, 0.78, 0.03];
$prediction = $model->predict($features);
```

Если вы четко понимаете, что означает каждый элемент этого массива и в каком пространстве он находится, половина проблем машинного обучения для вас уже решена.

### **Итоговое ощущение**

Векторы – это язык, на котором данные разговаривают с моделями. Размерность – это сложность этого языка. Пространство признаков – это сцена, на которой разворачивается все машинное обучение. Для PHP-разработчика важно перестать видеть в этом абстрактную математику и начать видеть хорошо структурированные массивы чисел, за которыми стоят реальные свойства реальных объектов.

Вся дальнейшая математика машинного обучения строится вокруг этой геометрической интерпретации: данные — это точки, модель — это поверхность, а обучение — процесс поиска формы, которая лучше всего разделяет или аппроксимирует эти точки.

[^1]: Feature engineering (инженерия или конструирование признаков) - это процесс преобразования сырых данных в информативные характеристики (признаки/фичи)

[^2]: One-hot encoding (прямое кодирование) — это метод преобразования категориальных данных в числовой формат для ML-моделей. Он создает новый бинарный столбец для каждой уникальной категории, ставя «1» при наличии значения и «0» в остальных случаях.

[^3]: Аффинное преобразование — это вид геометрической трансформации (поворот, масштабирование, сдвиг, перенос), сохраняющий параллельность линий, отношение длин отрезков и плоскостность. Оно сочетает линейное преобразование и перенос, часто применяется в 3D-графике, компьютерном зрении и машинном обучении для изменения объектов.

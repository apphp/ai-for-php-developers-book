# Кейс 1. Логистическая регрессия для оттока клиентов

#### Pure PHP: обучение логистической регрессии

Каждый пользователь описывается тремя признаками:

x = \[logins, avgSession, daysFromSignup]

Целевая переменная y равна 1, если пользователь ушел, и 0 иначе.

```php
function sigmoid(float $z): float
{
    return 1.0 / (1.0 + exp(-$z));
}

function dot(array $a, array $b): float
{
    $sum = 0.0;
    foreach ($a as $i => $v) {
        $sum += $v * $b[$i];
    }
    return $sum;
}

$X = [
    [1.0, 5.2, 30.0],
    [12.0, 15.1, 400.0],
    [3.0, 4.8, 60.0],
    [20.0, 25.0, 800.0],
];

$y = [1, 0, 1, 0];

$weights = [0.0, 0.0, 0.0];
$bias = 0.0;

$learningRate = 0.01;
$epochs = 1000;

for ($epoch = 0; $epoch < $epochs; $epoch++) {
    foreach ($X as $i => $x) {
        $z = dot($weights, $x) + $bias;
        $p = sigmoid($z);

        // градиент log loss
        $error = $p - $y[$i];

        foreach ($weights as $j => $w) {
            $weights[$j] -= $learningRate * $error * $x[$j];
        }

        $bias -= $learningRate * $error;
    }
}

$newUser = [5.0, 7.0, 120.0];
$probability = sigmoid(dot($weights, $newUser) + $bias);

echo "Вероятность ухода: " . round($probability, 3) . PHP_EOL;
```

Этот пример намеренно прост. Он показывает, что логистическая регрессия – это линейная модель плюс сигмоида и правильная функция потерь.

#### RubixML: тот же кейс короче и практичнее

В реальных проектах удобнее использовать готовые реализации.

```php
use Rubix\ML\Classifiers\LogisticRegression;
use Rubix\ML\Datasets\Labeled;

$samples = [
    [1, 5.2, 30],
    [12, 15.1, 400],
    [3, 4.8, 60],
    [20, 25.0, 800],
];

$labels = [1, 0, 1, 0];

$dataset = new Labeled($samples, $labels);

$classifier = new LogisticRegression(0.01, 1000);
$classifier->train($dataset);

$prediction = $classifier->predict([[5, 7, 120]]);

print_r($prediction);
```

RubixML скрывает детали оптимизации, но концептуально это та же самая модель: линейная комбинация признаков, сигмоида и decision boundary.

#### Визуализация decision boundary для кейса оттока

Чтобы сделать поведение модели наглядным, полезно временно упростить задачу и оставить только два признака. Например:

– количество входов за последний месяц,

– число дней с момента регистрации.

Среднюю длительность сессии можно зафиксировать или исключить из визуализации. Тогда каждый пользователь становится точкой на плоскости признаков.

В этом случае decision boundary логистической регрессии задается уравнением:

w₁·x₁ + w₂·x₂ + b = 0

Геометрически это прямая, которая делит пространство на две области. По одну сторону находятся пользователи с вероятностью ухода больше 0.5, по другую – с вероятностью меньше 0.5.

Важно подчеркнуть: хотя граница линейна, вероятность меняется плавно. Чем дальше точка от границы, тем увереннее модель в своем прогнозе.

<mark style="color:$info;">\[IMAGE 4: churn\_decision\_boundary]</mark>

<mark style="color:$info;">Промпт для изображения</mark>

<mark style="color:$info;">“Двумерный scatter plot пользователей сервиса подписки. Ось X – количество входов за месяц, ось Y – дни с момента регистрации. Точки двух классов: ушел (красные) и остался (синие). Прямая decision boundary логистической регрессии. Полупрозрачный градиент вероятности по обе стороны линии. Учебный стиль, чистый белый фон.”</mark>

Такая визуализация хорошо иллюстрирует ключевую идею логистической регрессии: модель линейно разделяет пространство признаков, но результат интерпретируется через вероятность, а не через жесткое правило.

> В более высоких размерностях decision boundary перестает быть линией, но логика остается той же – это гиперплоскость в пространстве признаков.

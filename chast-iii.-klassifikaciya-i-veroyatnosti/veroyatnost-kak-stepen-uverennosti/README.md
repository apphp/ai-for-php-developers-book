---
description: Частоты, апостериорные вероятности.
---

# Вероятность как степень уверенности

### Вероятность как степень уверенности

Когда разработчики слышат слово "вероятность", в голове часто всплывают игральные кости, подбрасывание монетки и школьная формула "благоприятные исходы делить на все возможные". Это полезная, но очень узкая картинка. В машинном обучении и в прикладной аналитике вероятность почти всегда означает другое – степень нашей уверенности в утверждении, исходя из имеющихся данных.

Эта глава нужна, чтобы аккуратно сдвинуть мышление: от вероятности как свойства мира к вероятности как модели нашего знания о мире.

### Классическая интуиция: вероятность как частота

Начнем с привычного. Если мы много раз подбрасываем честную монетку, то ожидаем, что орел будет выпадать примерно в половине случаев. Формально это записывается так:

$$
P(\text{орел}) = \frac{1}{2}
$$

Здесь вероятность интерпретируется как предел относительной частоты при большом числе экспериментов. Это так называемая частотная (frequentist) интерпретация.

Она хорошо работает там, где:

* эксперимент можно повторять бесконечно;
* условия не меняются;
* объект стабилен (монетка сегодня и завтра "та же самая").

Но уже на этом этапе возникает вопрос: а что делать с единичными событиями?

Какова вероятность того, что завтра пойдет дождь? Или что пользователь кликнет на кнопку? Или что письмо – это спам?

Мы не можем "прокрутить мир" миллион раз и посмотреть частоты. И вот здесь появляется другая, гораздо более полезная для разработчика интерпретация.

### Вероятность как степень уверенности

В байесовском смысле вероятность – это число, которое отражает нашу уверенность в утверждении, исходя из информации, которой мы располагаем.

Например:

* вероятность дождя 70% означает не то, что дождь "пойдет на 70%";
* это означает: при текущих данных и модели мы довольно уверены, что дождь будет.

То же самое в ML:

* модель говорит, что письмо – спам с вероятностью 0.92;
* это значит, что при всех известных признаках модель почти уверена в принадлежности письма к этому классу.

Важно: вероятность здесь не свойство объекта, а свойство нашей модели и нашего знания. И при другой модели или других данных эта оценка могла бы быть иной.

Это ключевой сдвиг мышления.

### Формально: вероятность как число от 0 до 1

Математически вероятность – это просто число:

$$
0 \le P(A) \le 1
$$

где:

* P(A) = 0 означает полную уверенность, что событие не произойдет;
* P(A) = 1 – полную уверенность, что произойдет;
* промежуточные значения – степень уверенности.

В байесовской интерпретации такие значения означают догматическую уверенность и на практике почти не используются.

Но в машинном обучении мы почти всегда работаем именно с такими значениями. Модель не говорит "да" или "нет", она говорит "насколько я уверена".

### Пример из жизни: медицинский тест

Представим, что у нас есть некоторый тест на заболевание. Он не идеален.

* Если человек болен, тест положительный в 99% случаев.
* Если человек здоров, тест отрицательный в 95% случаев.
* Болезнь встречается у 1 человека из 1000.

Если тест показал "положительно", какова вероятность, что человек действительно болен?

Интуиция часто подводит. Кажется: "почти 99%" Но реальный ответ будет сильно меньше из-за редкости болезни. Ниже мы увидим насколько.

Это классический пример того, почему вероятность – это не просто свойство теста, а результат обновления уверенности с учетом контекста.

И здесь впервые появляется формула Байеса.

### Немного математики: формула Байеса

Давайте посмотрим на эту формулу:<br>

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$



Где:

* $$A$$ – гипотеза (человек болен);
* $$B$$ – наблюдение (тест положительный);
* $$P(A)$$ – априорная вероятность (наша уверенность до теста);
* $$P(B)$$ – вероятность наблюдения, служит нормирующим коэффициентом;
* $$P(B \mid A)$$ – вероятность наблюдения при верной гипотезе (вероятность положительного теста, если человек действительно болен; чувствительность теста).
* $$P(A \mid B)$$ – апостериорная вероятность (уверенность после теста).

В машинном обучении эта логика используется постоянно, даже если формула явно не выписывается. Большинство практических моделей классификации по сути обновляют нашу уверенность на основе признаков.

<details>

<summary><strong>Медицинский тест. Подсчёт по формуле Байеса</strong></summary>

Посчитаем аккуратно, по формуле Байеса.

**Дано:**

* Распространённость болезни:\
  ( $$P(Б) = 1/1000 = 0.001$$ )
* Чувствительность теста (true positive):\
  ( $$P(+ \mid Б) = 0.99$$ )
* Специфичность теста (true negative):\
  ( $$P(- \mid З) = 0.95$$ )\
  ⇒ ложноположительный результат:\
  ( $$P(+ \mid З) = 0.05$$ )

**Шаг 1. Вероятность положительного теста**

$$P(+) = P(+ \mid Б)P(Б) + P(+ \mid З)P(З)$$&#x20;

$$P(+) = 0.99 \cdot 0.001 + 0.05 \cdot 0.999$$&#x20;

$$P(+) = 0.00099 + 0.04995 = 0.05094$$&#x20;

**Шаг 2. Вероятность, что человек болен при положительном тесте**

$$P(Б \mid +) = \frac{P(+ \mid Б)P(Б)}{P(+)}$$

$$P(Б \mid +) = \frac{0.99 \cdot 0.001}{0.05094}$$

$$P(Б \mid +) \approx 0.0194$$

**Ответ**

**≈ 1.94%**

***

**Интуитивная проверка "на пальцах"**

Возьмём 100000 человек:

* Больны: 100
  * Положительный тест: 99
* Здоровы: 99900
  * Ложноположительный тест: ≈ 4995

Всего положительных тестов:\
99 + 4995 = 5094

Доля реально больных среди положительных:

$$99 / 5 094 \approx 1.94\%$$

**Вывод**

Даже при **очень хорошем тесте**, если болезнь редкая, **положительный результат ≠ высокая вероятность болезни**.

Это классический пример того, как **базовая вероятность (base rate)** ломает интуицию.

</details>

### Визуальная интуиция вероятности

Вероятность удобно представлять не как абстрактное число, а как "массу уверенности", распределенную между вариантами.

Например, модель классификации текста может сказать:

* спам: 0.85
* не спам: 0.15

Это не просто два числа. Это распределение нашей уверенности между гипотезами.

<div align="left"><figure><img src="../../.gitbook/assets/13.1-probability-distribution-between-two-classes.png" alt="" width="563"><figcaption><p>13.1 Распределение вероятности между двумя классами</p></figcaption></figure></div>

#### Вероятность и softmax

Во многих моделях (логистическая регрессия, нейросети) на выходе используется функция softmax:

$$
P(y = i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
$$

Она превращает произвольные числа (оценки модели) в корректные вероятности:

* все значения от 0 до 1;
* сумма равна 1.

Хотя строго говоря, это вероятности модели, а не объективные вероятности мира.&#x20;

Смысл этого шага в том, чтобы привести оценки модели к форме степеней уверенности, с которыми удобно работать дальше – сравнивать, устанавливать пороговые значения и интерпретировать.

#### Пример вероятности с softmax

Во многих моделях машинного обучения выходом являются не вероятности, а так называемые _оценки_ (logits). Это просто числа, отражающие относительную уверенность модели в каждом варианте. Они могут быть любыми – положительными, отрицательными, большими или маленькими – и сами по себе не интерпретируются как вероятность.

Чтобы превратить такие оценки в корректные вероятности, используется функция `softmax`.

Рассмотрим простой пример. Пусть модель оценивает входящее письмо и выдает следующие оценки для трех классов:

```php
$scores = [
    'spam'   => 2.1,
    'promo'  => 1.3,
    'normal' => 0.2,
];
```

Эти числа не являются вероятностями. Их сумма не равна 1, и они могут свободно выходить за диапазон от 0 до 1.

Реализуем функцию `softmax` на чистом PHP:

```php
function softmax(array $scores): array
{
    // Для числовой стабильности вычитаем максимальное значение
    $max = max($scores);

    $expValues = [];
    $sum = 0.0;

    foreach ($scores as $key => $value) {
        $exp = exp($value - $max);
        $expValues[$key] = $exp;
        $sum += $exp;
    }

    $probabilities = [];
    foreach ($expValues as $key => $value) {
        $probabilities[$key] = $value / max($sum, 1);
​    }

    return $probabilities;
}
```

Применим `softmax` к оценкам модели:

```php
$probabilities = softmax($scores);

foreach ($probabilities as $class => $probability) {
    echo $class . ': ' . round($probability, 3) . PHP_EOL;
}

// Результат:
// spam: 0.625
// promo: 0.281
// normal: 0.094
```

Теперь мы получили корректное распределение вероятностей:

* каждое значение находится в диапазоне от 0 до 1;
* сумма всех значений равна 1;
* числа можно интерпретировать как степень уверенности модели.

Важное замечание: `softmax` не делает модель умнее. Он лишь переводит ее внутренние оценки в форму, удобную для интерпретации и принятия решений. Модель по-прежнему сомневается и распределяет уверенность между альтернативами, а не выдает жесткий ответ "да" или "нет".

<div align="left"><figure><img src="../../.gitbook/assets/13.3-сonverting-logits-to-probabilities-via-softmax.png" alt="" width="563"><figcaption><p>13.3 softmax превращает logits в вероятности</p></figcaption></figure></div>

### Почему вероятность почти никогда не бывает 0 или 1

В реальных данных почти всегда есть шум, неполнота и неизвестные факторы. Поэтому хорошие модели крайне редко выдают вероятность ровно 0 или ровно 1.

Если модель говорит "100% уверенности", это обычно тревожный сигнал:

* переобучение;
* утечка данных;
* слишком агрессивные предположения.

Здоровая модель почти всегда оставляет место сомнению.

<div align="left"><figure><img src="../../.gitbook/assets/12.2-confidence-scale-from-0-to-1.png" alt="" width="563"><figcaption><p>13.2 Шкала уверенности от 0 до 1</p></figcaption></figure></div>

### Вероятность и решения

Не менее важно понимать: вероятность сама по себе ничего не решает. Решения принимает бизнес-логика.

Например:

* письмо – спам с вероятностью 0.6;
* удалять ли его автоматически? зависит от цены ошибки.

Поэтому в прикладных системах вероятность – это вход в принятие решений, а не само решение.

### Итог

Вероятность в машинном обучении – это не про монетки и кубики. Это язык, на котором модель говорит о своей уверенности.

Она:

* отражает наше знание, а не объективную истину;
* обновляется при поступлении новых данных;
* почти всегда содержит неопределенность;
* служит основой для принятия решений, но не подменяет их.

Если держать эту интуицию в голове, формулы и алгоритмы начинают выглядеть гораздо более человечными – и гораздо менее мистическими.

{% hint style="info" %}
Чтобы самостоятельно протестировать этот код, установите примеры из официального репозитория [GitHub](https://github.com/apphp/ai-for-php-developers-examples) или воспользуйтесь [онлайн-демонстрацией](https://aiwithphp.org/books/ai-for-php-developers/examples/part-3/probability-as-degree-of-confidence) для его запуска.
{% endhint %}
